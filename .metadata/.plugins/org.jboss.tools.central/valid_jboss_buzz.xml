<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>AI software stack inspection with Thoth and TensorFlow</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lvnLRySjp0s/" /><category term="Big Data" /><category term="CI/CD" /><category term="Machine Learning" /><category term="Performance" /><category term="data science" /><category term="human machine interaction" /><category term="Jupyter notebook" /><category term="Project Thoth" /><category term="software stack analysis" /><category term="TensorFlow" /><author><name>Francesco Murdaca</name></author><id>https://developers.redhat.com/blog/?p=783187</id><updated>2020-09-30T08:02:04Z</updated><published>2020-09-30T08:02:04Z</published><content type="html">&lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/core/blob/master/doc/ROADMAP.md#thoth-roadmap"&gt;Project Thoth&lt;/a&gt; develops &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; tools that enhance the day-to-day life of developers and data scientists. Thoth uses machine-generated knowledge to boost the performance, security, and quality of your applications using &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence&lt;/a&gt; (AI) through &lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning."&gt;reinforcement learning (RL)&lt;/a&gt;. This machine-learning approach is implemented in &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser"&gt;Thoth adviser&lt;/a&gt; (if you want to know more, &lt;a target="_blank" rel="nofollow" href="https://www.youtube.com/watch?v=WEJ65Rvj3lc&amp;#38;t=1s"&gt;click here&lt;/a&gt;) and it is used by &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/integration.rst"&gt;Thoth integrations&lt;/a&gt; to provide the software stack based on &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/thamos#using-custom-configuration-file-template"&gt;user inputs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, I introduce a case study—a recent inspection of a runtime issue when importing TensorFlow 2.1.0—to demonstrate the human-machine interaction between the Thoth team and Thoth components. By following the case study from start to finish, you will learn how Thoth gathers and analyzes some of the data to provide &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/justifications"&gt;advice&lt;/a&gt; to its users, including bots such as &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/kebechet#kebechet"&gt;Kebechet&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://github.com/AICoE/aicoe-ci"&gt;AI-backed continuous integration pipelines&lt;/a&gt;, and developers using &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/qeb-hwt"&gt;GitHub apps&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Both the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station"&gt;Thoth machinery&lt;/a&gt; and team rely on bots and automated pipelines running on Red Hat OpenShift. Thoth takes a variety of inputs to determine the correct advice:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/solver"&gt;Solver&lt;/a&gt;, which Thoth uses to discover if something can be installed in a particular runtime environment, such as &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8 with Python 3.6.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/tree/master/notebooks/thoth-security-dataset#thoth-security-datasets"&gt;Security indicators&lt;/a&gt; that uncover vulnerabilities of a different nature, which can be applied to security advice.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/mi"&gt;Project meta information&lt;/a&gt;, such as project-maintenance status or development-process behavior that affects the overall project.&lt;/li&gt; &lt;li&gt;Inspections, which Thoth uses to discover code quality issues or performance across packages.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article focuses on inspections. I will show you the results from an automated software stack inspection run through &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;&amp;#8216;s &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/dependency_monkey.rst"&gt;Dependency Monkey&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/amun-api#amun-service"&gt;Amun&lt;/a&gt; components. Thoth uses automated inspections to introduce new advice about software stacks for Thoth users. Another way to integrate advice could be via automated pipelines that can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Boost performance&lt;/li&gt; &lt;li&gt;Optimize &lt;a href="https://developers.redhat.com/blog/category/machine-learning/"&gt;machine learning&lt;/a&gt; (ML) model inference&lt;/li&gt; &lt;li&gt;Ensure that there are no failures during the model runtime (for example, during inference)&lt;/li&gt; &lt;li&gt;Avoid using software stacks that does not guarantee security.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Thoth components: Amun and Dependency Monkey&lt;/h2&gt; &lt;p&gt;Given the list of packages that should be installed and the hardware requested to run the application, &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/amun-api#amun-service"&gt;Amun&lt;/a&gt; executes the requested application stack in the requested environment. Amun acts as an execution engine for Thoth. Applications are then built and tested using &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance"&gt;Thoth Performance Indicators (PI)&lt;/a&gt;. See Amun&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/amun-api/blob/master/README.rst"&gt;README documentation&lt;/a&gt; for more information about this service.&lt;/p&gt; &lt;p&gt;Another Thoth component, &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/docs/source/dependency_monkey.rst"&gt;Dependency Monkey&lt;/a&gt;, can be used to schedule Amun. Dependency Monkey was designed to automate the evaluation of certain aspects of a software stack, such as code quality or performance. Therefore, it aims to automatically verify software stacks and aggregate relevant observations.&lt;/p&gt; &lt;p&gt;From these two components, the Thoth team created &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/thoth-performance-dataset"&gt;Thoth Performance Datasets&lt;/a&gt;, which contains observations about performance for software stacks. For example, Thoth Performance Datasets could use &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/conv2d.py"&gt;PIconv2d&lt;/a&gt; to obtain performance data for different application types (such as machine learning) and code quality. It could then use a performance indicator like &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/import.py"&gt;PiImport&lt;/a&gt; to discover errors during an application run.&lt;/p&gt; &lt;h2&gt;Transparent and reproducible datasets&lt;/h2&gt; &lt;p&gt;In the spirit of open source, the Thoth team wants to guarantee that the datasets and knowledge that we collect and use are transparent and reproducible. Machine learning models, such as the reinforcement learning model leveraged by &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser"&gt;Thoth Adviser&lt;/a&gt;, should be as transparent as the datasets they are working on.&lt;/p&gt; &lt;p&gt;For transparency, we&amp;#8217;ve introduced &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets#thoth-datasets"&gt;Thoth Datasets&lt;/a&gt;, where we share the notebooks that we used to analyze a data collection and all of the results. We encourage anyone interested in the topic to use Thoth Datasets to verify our findings or for other purposes.&lt;/p&gt; &lt;p&gt;For reproducibility, we&amp;#8217;ve introduced &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/dependency-monkey-zoo"&gt;Dependency Monkey Zoo&lt;/a&gt;, where we collect all of the specifications used to run an analysis. Having all of the specs in one place allows us to reproduce the results of a study. Anyone can use the specs to perform similar studies in different environments for comparison.&lt;/p&gt; &lt;h2&gt;Case study: Automated software stack inspection for TensorFlow 2.1.0&lt;/h2&gt; &lt;p&gt;For this case study, we will use Thoth&amp;#8217;s Amun and Dependency Monkey components to automatically produce data. We&amp;#8217;ll then introduce reusable &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/templates/Amun%20Inspection%20Analysis%20Template.ipynb"&gt;Jupyter notebook templates&lt;/a&gt; to extract specific information from the datasets. Finally, we&amp;#8217;ll create new advice based on the results.&lt;/p&gt; &lt;p&gt;The human side of this human-machine interaction focuses on assessing the quality of the results and formulating the advice. The rest of the process is machine-automated. Automation makes the process easy to repeat to produce a new source of information for analysis.&lt;/p&gt; &lt;p&gt;In the next sections, I introduce the initial problem, then describe the analysis performed and the resulting new advice for Thoth users.&lt;/p&gt; &lt;h2&gt;Initial request&lt;/h2&gt; &lt;p&gt;Our goal with this inspection is to analyze build- and runtime failures when importing TensorFlow 2.1.0 and use these to derive observations about the quality of the software stack.&lt;/p&gt; &lt;p&gt;For this analysis, Dependency Monkey sampled the state space of all of the possible &lt;code&gt;TensorFlow==2.1.0&lt;/code&gt; stacks (from upstream builds). For inspection purposes, we built and ran the application using the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/matmul.py"&gt;PiMatmul&lt;/a&gt; performance indicator.&lt;/p&gt; &lt;p&gt;The sections below detail the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/dependency-monkey-zoo/tree/master/tensorflow/inspection-2020-09-04"&gt;Dependency Monkey inspection results&lt;/a&gt; and the resulting &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/notebooks/issues/70"&gt;analysis&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The first analysis&lt;/h2&gt; &lt;p&gt;From the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/thoth-performance-dataset/PerformanceTensorFlow2.1.0SoftwareStackCombinations.ipynb"&gt;software stack analysis&lt;/a&gt; of inspection results, we discovered that TensorFlow 2.1.0 was giving errors during approximately 50% of inspections during a run. The error is shown in the following output from the Jupyter Notebook:&lt;/p&gt; &lt;pre&gt;'2020-09-05 07:14:36.333589: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libnvinfer.so.6\'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory 2020-09-05 07:14:36.333811: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library \'libnvinfer_plugin.so.6\'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory 2020-09-05 07:14:36.333844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. /opt/app-root/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters /opt/app-root/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.5) or chardet (2.3.0) doesn\'t match a supported version! RequestsDependencyWarning) Traceback (most recent call last): File "/home/amun/script", line 14, in &amp;#60;module&amp;#62; import tensorflow as tf File "/opt/app-root/lib/python3.6/site-packages/tensorflow/__init__.py", line 101, in &amp;#60;module&amp;#62; from tensorflow_core import * File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/__init__.py", line 40, in &amp;#60;module&amp;#62; from tensorflow.python.tools import module_util as _module_util File "/opt/app-root/lib/python3.6/site-packages/tensorflow/__init__.py", line 50, in __getattr__ module = self._load() File "/opt/app-root/lib/python3.6/site-packages/tensorflow/__init__.py", line 44, in _load\n module = _importlib.import_module(self.__name__) File "/opt/app-root/lib64/python3.6/importlib/__init__.py", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/__init__.py", line 95, in &amp;#60;module&amp;#62; from tensorflow.python import keras File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py", line 27, in &amp;#60;module&amp;#62; from tensorflow.python.keras import models File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/__init__.py", line 27, in &amp;#60;module&amp;#62; from tensorflow.python.keras import models File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py", line 25, in &amp;#60;module&amp;#62; from tensorflow.python.keras.engine import network File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py", line 46, in &amp;#60;module&amp;#62; from tensorflow.python.keras.saving import hdf5_format File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py", line 32, in &amp;#60;module&amp;#62; from tensorflow.python.keras.utils import conv_utils File "/opt/app-root/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/conv_utils.py", line 22, in &amp;#60;module&amp;#62; from six.moves import range # pylint: disable=redefined-builtin ImportError: cannot import name \'range\''&lt;/pre&gt; &lt;p&gt;Specifically, we could see that some combinations of &lt;code&gt;six&lt;/code&gt; and &lt;code&gt;urllib3&lt;/code&gt; produced that error, as described in the following &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/notebooks/issues/70#issuecomment-688656575"&gt;output&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;============================================= urllib3 ============================================= In successfull inspections: ['urllib3-1.10.4-pypi-org' 'urllib3-1.16-pypi-org' 'urllib3-0.3-pypi-org' 'urllib3-1.21.1-pypi-org' 'urllib3-1.25.1-pypi-org' 'urllib3-1.25-pypi-org' 'urllib3-1.18.1-pypi-org' 'urllib3-1.24.1-pypi-org' 'urllib3-1.10.1-pypi-org' 'urllib3-1.10.3-pypi-org' 'urllib3-1.25.7-pypi-org' 'urllib3-1.10-pypi-org' 'urllib3-1.7.1-pypi-org' 'urllib3-1.13-pypi-org' 'urllib3-1.19.1-pypi-org' 'urllib3-1.11-pypi-org' 'urllib3-1.10.2-pypi-org' 'urllib3-1.15.1-pypi-org' 'urllib3-1.25.3-pypi-org' 'urllib3-1.13.1-pypi-org' 'urllib3-1.21-pypi-org' 'urllib3-1.17-pypi-org' 'urllib3-1.23-pypi-org'] In failed inspections: ['urllib3-1.5-pypi-org'] In failed inspections but not in successfull: {'urllib3-1.5-pypi-org'} In failed inspections and in successfull: set() ============================================= six ============================================= In successfull inspections: ['six-1.13.0-pypi-org' 'six-1.12.0-pypi-org'] In failed inspections: ['six-1.13.0-pypi-org' 'six-1.12.0-pypi-org'] In failed inspections but not in successfull: set() In failed inspections and in successfull: {'six-1.13.0-pypi-org', 'six-1.12.0-pypi-org'}&lt;/pre&gt; &lt;p&gt;Therefore, we discovered that &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/urllib3/"&gt;&lt;strong&gt;urllib3&lt;/strong&gt;&lt;/a&gt; library releases were the same across all failed inspections but not in any of the successful inspections, while &lt;a target="_blank" rel="nofollow" href="https://pypi.org/project/six/"&gt;&lt;strong&gt;six&lt;/strong&gt;&lt;/a&gt; library releases didn&amp;#8217;t show any differences between failed and successful once.&lt;/p&gt; &lt;h2&gt;The second analysis&lt;/h2&gt; &lt;p&gt;For our next step, we decided to run another analysis to restrict the cases. For this run, we used a newly created performance indicator called &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/import.py"&gt;PiImport&lt;/a&gt; as shown in Table 1.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 1: The PiImport performance indicator.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Dependency Monkey sampled the state space of all the possible &lt;code&gt;TensorFlow==2.1.0&lt;/code&gt; stacks (from upstream builds). The application was built and run using the &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/performance/blob/master/tensorflow/import.py"&gt;PiImport&lt;/a&gt; performance indicator.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Specification  &lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/dependency-monkey-zoo/tree/master/tensorflow/inspection-2020-09-08.1"&gt;Dependency Monkey specification&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Identify specific versions that fail to produce final &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/pull/1172"&gt;advice&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/issues/16"&gt;Issue&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Results of the second analysis&lt;/h2&gt; &lt;p&gt;From the new &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/datasets/blob/master/notebooks/thoth-performance-dataset/PerformanceTensorFlow2.1.0SoftwareStackCombinationsErrors.ipynb"&gt;analysis&lt;/a&gt;, we were able to identify all of the specific versions of &lt;code&gt;urllib3&lt;/code&gt; and &lt;code&gt;six&lt;/code&gt; that did not work together and that were causing issues during runtime. The output in Figure 1 shows the incompatible versions of the two packages.&lt;/p&gt; &lt;p&gt;dFigure 1: Identifying the incompatible versions of urllib3 and six that do not allow to run Tensorflow 2.1.0.&lt;/p&gt; &lt;h2&gt;The advice&lt;/h2&gt; &lt;p&gt;All of this backtracing led to an &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/pull/1172"&gt;adviser step&lt;/a&gt; called &lt;a target="_blank" rel="nofollow" href="https://github.com/thoth-station/adviser/blob/master/thoth/adviser/steps/tensorflow_21_urllib3.py"&gt;TensorFlow21Urllib3Step&lt;/a&gt;. With this step, we can penalize software stacks containing the specific version of &lt;code&gt;urllib3&lt;/code&gt; that cause runtime issues when attempting to import TensorFlow 2.1.0. The following prediction, created by Thoth, results in a higher quality software stack for users.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 2: The TensorFlow21Urllib3Step adviser step.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;Title&lt;/b&gt;&lt;/td&gt; &lt;td&gt;TensorFlow in version 2.1 can cause runtime errors when imported, caused by incompatibility between &lt;code&gt;urllib3&lt;/code&gt; and &lt;code&gt;six&lt;/code&gt; packages.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;Issue description&lt;/b&gt;&lt;/td&gt; &lt;td&gt;Package &lt;code&gt;urllib3&lt;/code&gt; in some versions is shipped with a bundled version of &lt;code&gt;six&lt;/code&gt;, which has its own mechanism for imports and import context handling. Importing &lt;code&gt;urllib3&lt;/code&gt; in the TensorFlow codebase causes initialization of the bundled &lt;code&gt;six&lt;/code&gt; module, which collides with a subsequent import from unbundled &lt;code&gt;six&lt;/code&gt; modules.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;You can find the complete issue description, and the recommended resolution, &lt;a target="_blank" rel="nofollow" href="https://thoth-station.ninja/j/tf_21_urllib3.html"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#38;linkname=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F30%2Fai-software-stack-inspection-with-thoth-and-tensorflow%2F&amp;#038;title=AI%20software%20stack%20inspection%20with%20Thoth%20and%20TensorFlow" data-a2a-url="https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/" data-a2a-title="AI software stack inspection with Thoth and TensorFlow"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/"&gt;AI software stack inspection with Thoth and TensorFlow&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lvnLRySjp0s" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Project Thoth develops open source tools that enhance the day-to-day life of developers and data scientists. Thoth uses machine-generated knowledge to boost the performance, security, and quality of your applications using artificial intelligence (AI) through reinforcement learning (RL). This machine-learning approach is implemented in Thoth adviser (if you want to know more, click here) and [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/"&gt;AI software stack inspection with Thoth and TensorFlow&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">783187</post-id><dc:creator>Francesco Murdaca</dc:creator><dc:date>2020-09-30T08:02:04Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/30/ai-software-stack-inspection-with-thoth-and-tensorflow/</feedburner:origLink></entry><entry><title>Quicker, easier GraphQL queries with Open Liberty 20.0.0.9</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CeJhWusLdiQ/" /><category term="Java" /><category term="Kubernetes" /><category term="Microservices" /><category term="Open source" /><category term="graphql mutation" /><category term="graphql query" /><category term="MicroProfile" /><category term="OpenLiberty" /><category term="openshift" /><author><name>Jakub Pomykala</name></author><id>https://developers.redhat.com/blog/?p=772447</id><updated>2020-09-29T07:00:48Z</updated><published>2020-09-29T07:00:48Z</published><content type="html">&lt;p&gt;Open Liberty 20.0.0.9 lets developers experiment with the type-safe SmallRye GraphQL Client API, and write and run GraphQL queries and mutations more easily with a built-in GraphiQL user interface (UI). This article introduces the new features and updates in Open Liberty 20.0.0.9:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#GraphQLAPIs"&gt;Experiment with a third-party GraphQL client API.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#GraphiQL"&gt;Use the built-in GraphiQL UI for faster queries and mutations.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#feedback"&gt;Give us your feedback!&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Run your apps using Open Liberty 20.0.0.9&lt;/h2&gt; &lt;p&gt;If you are using &lt;a target="_blank" rel="nofollow" href="https://openliberty.io//guides/maven-intro.html"&gt;Maven&lt;/a&gt;, use these coordinates to update to the newest version of Open Liberty:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.openliberty&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;openliberty-runtime&amp;#60;/artifactId&amp;#62; &amp;#60;version&amp;#62;20.0.0.9&amp;#60;/version&amp;#62; &amp;#60;type&amp;#62;zip&amp;#60;/type&amp;#62; &amp;#60;/dependency&amp;#62; &lt;/pre&gt; &lt;p&gt;For &lt;a target="_blank" rel="nofollow" href="https://openliberty.io//guides/gradle-intro.html"&gt;Gradle&lt;/a&gt;, enter:&lt;/p&gt; &lt;pre&gt;dependencies { libertyRuntime group: 'io.openliberty', name: 'openliberty-runtime', version: '[20.0.0.9,)' } &lt;/pre&gt; &lt;p&gt;If you&amp;#8217;re using Docker, it&amp;#8217;s:&lt;/p&gt; &lt;pre&gt;FROM open-liberty &lt;/pre&gt; &lt;h2 id="GraphQLAPIs"&gt;Experiment with a third-party GraphQL client API&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/microprofile-graphql"&gt;MicroProfile GraphQL&lt;/a&gt; has only been available in Open Liberty for a few months, and it is already a hit. That said, there are a few ways that we want to improve it and make it more complete. One feature that we want to improve is the GraphQL client API. While the official client API is not expected until the next release of MicroProfile GraphQL, you can start experimenting now with the type-safe SmallRye GraphQL client API.&lt;/p&gt; &lt;p&gt;SmallRye is the underlying implementation of MicroProfile GraphQL. You can access its above-and-beyond-the-spec features by adding the &amp;#8220;third-party&amp;#8221; API type visibility to your application:&lt;/p&gt; &lt;pre&gt;&amp;#60;application name="MyGraphQLApp" location="MyGraphQLApp.war"&amp;#62; &amp;#60;classloader apiTypeVisibility="+third-party"/&amp;#62; &amp;#60;/application&amp;#62; &lt;/pre&gt; &lt;p&gt;This update lets you access SmallRye GraphQL APIs like the type-safe client. Note that these APIs might change in future releases because SmallRye is continuously evolving. For more information, please visit &lt;a target="_blank" rel="nofollow" href="https://github.com/smallrye/smallrye-graphql"&gt;SmallRye GraphQL project&lt;/a&gt;. For Open Liberty 20.0.0.9, we are using SmallRye GraphQL 1.0.7.&lt;/p&gt; &lt;h3&gt;Type-safe invocation for remote methods&lt;/h3&gt; &lt;p&gt;The SmallRye GraphQL client APIs are very similar to &lt;a href="https://developers.redhat.com/cheat-sheets/microprofile-rest-client"&gt;MicroProfile Rest Client&lt;/a&gt;, which uses an interface to invoke remote methods in a type-safe manner. For example, suppose we want a client that can invoke a query of all of the superheroes in a given location. We would create a query interface like this:&lt;/p&gt; &lt;pre&gt;@GraphQlClientApi interface SuperHeroesApi { List allHeroesIn(String location); } &lt;/pre&gt; &lt;p&gt;Where &lt;code&gt;SuperHero&lt;/code&gt; on the client-side looks like this:&lt;/p&gt; &lt;pre&gt;class SuperHero { private String name; private List superPowers; } &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;SuperHero&lt;/code&gt; entity might contain dozens of fields on the server-side, but if we&amp;#8217;re only interested in the hero&amp;#8217;s name and superpowers, then we only need those two fields in our client-side class. Now, we can invoke the query with code like this:&lt;/p&gt; &lt;pre&gt;SuperHeroesApi api = GraphQlClientBuilder.newBuilder().build(SuperHeroesApi.class); List heroesOfNewYork = api.allHeroesIn("NYC"); &lt;/pre&gt; &lt;p&gt;Remember that this client API is not official, but the official MicroProfile GraphQL 1.1 API will be based on it. Think of this as a preview.&lt;/p&gt; &lt;h2 id="GraphiQL"&gt;Use the built-in GraphiQL UI for faster queries and mutations&lt;/h2&gt; &lt;p&gt;Open Liberty now sports a built-in &lt;a target="_blank" rel="nofollow" href="https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md"&gt;GraphiQL&lt;/a&gt; user interface as shown in Figure 1. The new, web-based UI allows you to write and execute GraphQL queries and mutations in real-time with advanced editing features like command completion, query history, schema introspection, and so on.&lt;/p&gt; &lt;div id="attachment_786817" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00.png"&gt;&lt;img aria-describedby="caption-attachment-786817" class="wp-image-786817 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-1024x391.png" alt="Web UI open to edit a query, with hover-over text displayed." width="640" height="244" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-1024x391.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-300x115.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/Screenshot-2020-09-22-at-10.50.00-768x293.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786817" class="wp-caption-text"&gt;Figure 1: Work faster with the GraphiQL web UI.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To enable the UI, you must first &lt;a target="_blank" rel="nofollow" href="https://openliberty.io/blog/2020/06/10/microprofile-graphql-open-liberty.html"&gt;write and deploy a MicroProfile GraphQL application&lt;/a&gt;. Then add this line to your &lt;code&gt;server.xml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&amp;#60;variable name="io.openliberty.enableGraphQLUI" value="true" /&amp;#62; &lt;/pre&gt; &lt;p&gt;You can use a web browser to access the UI, by merely opening your GraphQL application&amp;#8217;s context root and adding &lt;code&gt;/graphql-ui&lt;/code&gt;. As an example, suppose that we use the default port (9080) and our application is named&lt;code&gt;myGraphQLApp&lt;/code&gt;. In that case, we would access the UI at &lt;code&gt;http://localhost:9080/myGraphQLApp/graphql-ui&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This workaround was &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/open-liberty/issues/13201"&gt;resolved in issue #13201&lt;/a&gt;.&lt;/p&gt; &lt;h2 id="feedback"&gt;We want your feedback&lt;/h2&gt; &lt;p&gt;As an open source team, we love receiving feedback from Open Liberty users. A recent example is this comment, taken from Open Liberty &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/open-liberty/issues/13036"&gt;#13036&lt;/a&gt;): &amp;#8220;Hello, I am using microprofile-graphql on openliberty and everything goes well except for the exception whitelisting mechanism via microprofile config &amp;#8230;&amp;#8221;&lt;/p&gt; &lt;p&gt;Our MicroProfile GraphQL feature has only been generally available for a few months, so it&amp;#8217;s great to know that users are adopting it. We&amp;#8217;re also excited that some of you are already exploring the &amp;#8220;dark corners&amp;#8221; of exception handling and similar features.&lt;/p&gt; &lt;p&gt;While we dislike discovering that we let a bug slip through the cracks, we&amp;#8217;re eager to fix them when they do. If you find an issue or want to suggest an enhancement that would make your experience with Open Liberty better, please let us know. You can always reach us by &lt;a target="_blank" rel="nofollow" href="https://github.com/OpenLiberty/open-liberty/issues"&gt;opening an issue on GitHub&lt;/a&gt; or contacting us on Twitter at &lt;a target="_blank" rel="nofollow" href="https://twitter.com/OpenLibertyIO"&gt;@OpenLibertyIO&lt;/a&gt;. We&amp;#8217;re also available to chat online using &lt;a target="_blank" rel="nofollow" href="https://gitter.im/OpenLiberty/help"&gt;Gitter&lt;/a&gt; and on the &lt;a target="_blank" rel="nofollow" href="https://gitter.im/OpenLiberty/developer-experience"&gt;Open Liberty Developer Experience&lt;/a&gt; page.&lt;/p&gt; &lt;h2&gt;Try Open Liberty 20.0.0.8 in Red Hat Runtimes now&lt;/h2&gt; &lt;p&gt;Open Liberty is part of the Red Hat Runtimes offering and is available to &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/products/red-hat-runtimes"&gt;Red Hat Runtimes subscribers&lt;/a&gt;. To learn more about deploying Open Liberty applications to &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, see our &lt;i&gt;&lt;a href="https://openliberty.io/guides/cloud-openshift.html" target="_blank" rel="nofollow noopener noreferrer"&gt;Open Liberty guide: Deploying microservices to OpenShift&lt;/a&gt;&lt;/i&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#38;linkname=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F29%2Fquicker-easier-graphql-queries-with-open-liberty-20-0-0-9%2F&amp;#038;title=Quicker%2C%20easier%20GraphQL%20queries%20with%20Open%20Liberty%2020.0.0.9" data-a2a-url="https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/" data-a2a-title="Quicker, easier GraphQL queries with Open Liberty 20.0.0.9"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/"&gt;Quicker, easier GraphQL queries with Open Liberty 20.0.0.9&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CeJhWusLdiQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Open Liberty 20.0.0.9 lets developers experiment with the type-safe SmallRye GraphQL Client API, and write and run GraphQL queries and mutations more easily with a built-in GraphiQL user interface (UI). This article introduces the new features and updates in Open Liberty 20.0.0.9: Experiment with a third-party GraphQL client API. Use the built-in GraphiQL UI for [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/"&gt;Quicker, easier GraphQL queries with Open Liberty 20.0.0.9&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">772447</post-id><dc:creator>Jakub Pomykala</dc:creator><dc:date>2020-09-29T07:00:48Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/29/quicker-easier-graphql-queries-with-open-liberty-20-0-0-9/</feedburner:origLink></entry><entry><title>Call an existing REST service with Apache Camel K</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/TdDN8vPz2vY/" /><category term="Java" /><category term="Kubernetes" /><category term="Operator" /><category term="Serverless" /><category term="application integration" /><category term="Camel K" /><category term="jitpack" /><category term="OpenAPI" /><category term="openshift" /><author><name>Mary Cochran</name></author><id>https://developers.redhat.com/blog/?p=760597</id><updated>2020-09-28T07:00:41Z</updated><published>2020-09-28T07:00:41Z</published><content type="html">&lt;p&gt;With the &lt;a href="https://developers.redhat.com/blog/2020/06/18/camel-k-1-0-the-serverless-integration-platform-goes-ga/"&gt;release of Apache Camel K&lt;/a&gt;, it is possible to create and deploy integrations with existing applications that are quicker and more lightweight than ever. In many cases, calling an existing REST endpoint is the best way to connect a new system to an existing one. Take the example of a cafe serving coffee. What happens when the cafe wants to allow customers to use a delivery service like GrubHub? You would only need to introduce a single &lt;a href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k/"&gt;Camel K&lt;/a&gt; integration to connect the cafe and GrubHub systems.&lt;/p&gt; &lt;p&gt;In this article, I will show you how to create a &lt;a href="https://developers.redhat.com/videos/youtube/51x9BewGCYA"&gt;Camel K integration&lt;/a&gt; that calls an existing REST service and uses its existing data format. For the data format, I have a Maven project configured with &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; objects. Ideally, you would have this packaged and available in a Nexus repository. For the purpose of my demonstration, I utilized &lt;a target="_blank" rel="nofollow" href="https://jitpack.io/"&gt;JitPack&lt;/a&gt;, which lets me have my dependency available in a repository directly from my GitHub code. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/jeremyrdavis/quarkus-cafe-demo/tree/kamel-1.0.0/grubhub-cafe-core"&gt;GitHub repository associated with this demo&lt;/a&gt; for the data format code and directions for getting it into JitPack.&lt;br /&gt; &lt;span id="more-760597"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;In order to follow the demonstration, you will need the following installed in your development environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;oc&lt;/code&gt; command-line tools&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/apache/camel-k/releases"&gt;Camel K client 1.0.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;A &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift 4.4 cluster&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Create the Camel K route&lt;/h2&gt; &lt;p&gt;First, we create the Camel K route file, which I have named &lt;code&gt;RestWithUndertow.java&lt;/code&gt;. Here, we open the file and create the class structure:&lt;/p&gt; &lt;pre&gt;public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { } }&lt;/pre&gt; &lt;p&gt;Next, we create the REST endpoint, and we also create the data formats that we will use. In this case, we&amp;#8217;ll receive the REST request as a &lt;code&gt;GrubHubOrder&lt;/code&gt;. We&amp;#8217;ll then transform it to a &lt;code&gt;CreateOrderCommand&lt;/code&gt;, which we&amp;#8217;ll send to the REST service that is already in use:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.model.rest.RestBindingMode; import com.redhat.quarkus.cafe.domain.CreateOrderCommand; import com.redhat.grubhub.cafe.domain.GrubHubOrder; import org.apache.camel.component.jackson.JacksonDataFormat; public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { JacksonDataFormat df = new JacksonDataFormat(CreateOrderCommand.class); rest() .post("/order").type(GrubHubOrder.class).consumes("application/json") .bindingMode(RestBindingMode.json) .produces("application/json") .to("direct:order"); } } &lt;/pre&gt; &lt;h2&gt;Create the data transformation&lt;/h2&gt; &lt;p&gt;Now we can create a method in the same file, which will assist with the data transformation from the &lt;code&gt;GrubHubOrder&lt;/code&gt; to the &lt;code&gt;CreateOrderCommand&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; public void transformMessage(Exchange exchange){ Message in = exchange.getIn(); GrubHubOrder gho = in.getBody(GrubHubOrder.class); List oi = gho.getOrderItems(); List list = new ArrayList(); for(GrubHubOrderItem i : oi){ LineItem li = new LineItem(Item.valueOf(i.getOrderItem()),i.getName()); list.add(li); } CreateOrderCommand coc = new CreateOrderCommand(list, null); in.setBody(coc); } &lt;/pre&gt; &lt;p&gt;Make sure that you add the following imports to the file, as well:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.Exchange; import org.apache.camel.Message; import com.redhat.quarkus.cafe.domain.LineItem; import com.redhat.quarkus.cafe.domain.Item; import java.util.List; import java.util.ArrayList; import com.redhat.grubhub.cafe.domain.GrubHubOrderItem; &lt;/pre&gt; &lt;h2&gt;Call the existing service from your Camel K REST endpoint&lt;/h2&gt; &lt;p&gt;Now that we have a method to do the transformation, we can implement the rest of the Camel K REST endpoint and make it call the existing service. Add the following below the code that you have so far:&lt;/p&gt; &lt;pre&gt; from("direct:order") .log("Incoming Body is ${body}") .log("Incoming Body after unmarshal is ${body}") .bean(this,"transformMessage") .log("Outgoing pojo Body is ${body}") .marshal(df) //transforms the java object into json .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader(Exchange.CONTENT_TYPE, constant("application/json")) .setHeader("Accept",constant("application/json")) .log("Body after transformation is ${body} with headers: ${headers}") .to("http://?bridgeEndpoint=true&amp;#38;throwExceptionOnFailure=false") .setHeader(Exchange.HTTP_RESPONSE_CODE,constant(200)) .transform().simple("{Order Placed}"); &lt;/pre&gt; &lt;p&gt;Note that this example includes plenty of logging to give visibility to what is being done. I have set the &lt;code&gt;BridgeEndpoint&lt;/code&gt; option to &lt;code&gt;true&lt;/code&gt;, which allows us to ignore the &lt;code&gt;HTTP_URI&lt;/code&gt; incoming header and use the full URL that we&amp;#8217;ve specified. This is important when taking an incoming REST request that will call a new one. You can read more about &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/components/latest/http-component.html"&gt;the &lt;code&gt;BridgeEndpoint&lt;/code&gt; option here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Save the route file&lt;/h2&gt; &lt;p&gt;Your complete Camel K route file should look something like this:&lt;/p&gt; &lt;pre&gt;import org.apache.camel.Exchange; import org.apache.camel.Message; import org.apache.camel.model.rest.RestBindingMode; import com.redhat.quarkus.cafe.domain.LineItem; import com.redhat.quarkus.cafe.domain.Item; import java.util.List; import java.util.ArrayList; import com.redhat.quarkus.cafe.domain.CreateOrderCommand; import com.redhat.grubhub.cafe.domain.GrubHubOrder; import com.redhat.grubhub.cafe.domain.GrubHubOrderItem; import org.apache.camel.component.jackson.JacksonDataFormat; public class RestWithUndertow extends org.apache.camel.builder.RouteBuilder { @Override public void configure() throws Exception { JacksonDataFormat df = new JacksonDataFormat(CreateOrderCommand.class); rest() .post("/order").type(GrubHubOrder.class).consumes("application/json") .bindingMode(RestBindingMode.json) .produces("application/json") .to("direct:order"); from("direct:order") .log("Incoming Body is ${body}") .log("Incoming Body after unmarshal is ${body}") .bean(this,"transformMessage") .log("Outgoing pojo Body is ${body}") .marshal(df) .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader(Exchange.CONTENT_TYPE, constant("application/json")) .setHeader("Accept",constant("application/json")) .log("Body after transformation is ${body} with headers: ${headers}") //need to change url after knowing what the cafe-web url will be .to("http://sampleurl.com?bridgeEndpoint=true&amp;#38;throwExceptionOnFailure=false") .setHeader(Exchange.HTTP_RESPONSE_CODE,constant(200)) .transform().simple("{Order Placed}"); } public void transformMessage(Exchange exchange){ Message in = exchange.getIn(); GrubHubOrder gho = in.getBody(GrubHubOrder.class); List oi = gho.getOrderItems(); List list = new ArrayList(); for(GrubHubOrderItem i : oi){ LineItem li = new LineItem(Item.valueOf(i.getOrderItem()),i.getName()); list.add(li); } CreateOrderCommand coc = new CreateOrderCommand(list, null); in.setBody(coc); } }&lt;/pre&gt; &lt;p&gt;Save this file and get ready for the last step.&lt;/p&gt; &lt;h2&gt;Run the integration&lt;/h2&gt; &lt;p&gt;To run your integration, you will need to have the Camel K Operator installed and ensure that it has access to the dependencies in JitPack. Do the following to get your infrastructure ready for the integration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Log in to OpenShift using the &lt;code&gt;oc&lt;/code&gt; command line.&lt;/li&gt; &lt;li&gt;Install the Camel K Operator via the OpenShift OperatorHub. The default options are fine.&lt;/li&gt; &lt;li&gt;Ensure you have &lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/cli/cli.html"&gt;Kamel CLI tooling &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Use the &lt;code&gt;oc&lt;/code&gt; and &lt;code&gt;kamel&lt;/code&gt; tools and the following command to create an integration platform that provides access to JitPack: &lt;pre&gt;kamel install --olm=false --skip-cluster-setup --skip-operator-setup --maven-repository https://jitpack.io@id=jitpack@snapshots &lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Once you see that everything is ready in your OpenShift console (you can always enter &lt;code&gt;oc get pods&lt;/code&gt;to check), deploy the integration. Using your Kamel tools again, make sure that you are logged into OpenShift and on the appropriate project, and run the following command:&lt;/p&gt; &lt;pre&gt;kamel run --name=rest-with-undertow --dependency=camel-jackson --dependency=mvn:com.github.jeremyrdavis:quarkus-cafe-demo:1.5-SNAPSHOT --dependency=mvn:com.github.jeremyrdavis.quarkus-cafe-demo:grubhub-cafe-core:1.5-SNAPSHOT --dependency=camel-openapi-java RestWithUndertow.java &lt;/pre&gt; &lt;p&gt;This command ensures that your route starts with all of the appropriate dependencies. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/jeremyrdavis/quarkus-cafe-demo/tree/kamel-1.0.0/camel-k-grub-hub"&gt;GitHub repository for this article&lt;/a&gt; for a complete,  working version of this code and the service that it calls.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Camel K allows you to develop your integrations quickly and efficiently while keeping your footprint small. Even when you have some dependencies for your integrations you can utilize the Knative technology in Camel K to make your integrations less resource intensive and allow for quicker deployments.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#38;linkname=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fcall-an-existing-rest-service-with-apache-camel-k%2F&amp;#038;title=Call%20an%20existing%20REST%20service%20with%20Apache%20Camel%20K" data-a2a-url="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/" data-a2a-title="Call an existing REST service with Apache Camel K"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/"&gt;Call an existing REST service with Apache Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/TdDN8vPz2vY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;With the release of Apache Camel K, it is possible to create and deploy integrations with existing applications that are quicker and more lightweight than ever. In many cases, calling an existing REST endpoint is the best way to connect a new system to an existing one. Take the example of a cafe serving coffee. What [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/"&gt;Call an existing REST service with Apache Camel K&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">760597</post-id><dc:creator>Mary Cochran</dc:creator><dc:date>2020-09-28T07:00:41Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/28/call-an-existing-rest-service-with-apache-camel-k/</feedburner:origLink></entry><entry><title>Build a data streaming pipeline using Kafka Streams and Quarkus</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6-PYLEG5zIc/" /><category term="Java" /><category term="Modern App Dev" /><category term="Quarkus" /><category term="Spring Boot" /><category term="Stream Processing" /><category term="data pipeline" /><category term="data streaming" /><category term="Kafka streams" /><category term="kafka tutorial" /><category term="message processing" /><author><name>Kapil Shukla</name></author><id>https://developers.redhat.com/blog/?p=749517</id><updated>2020-09-28T07:00:22Z</updated><published>2020-09-28T07:00:22Z</published><content type="html">&lt;p&gt;In typical data warehousing systems, &lt;a href="https://developers.redhat.com/blog/category/big-data/"&gt;data&lt;/a&gt; is first accumulated and then processed. But with the advent of new technologies, it is now possible to process data as and when it arrives. We call this real-time data processing. In real-time processing, data streams through pipelines; i.e., moving from one system to another. Data gets generated from static sources (like databases) or real-time systems (like transactional applications), and then gets filtered, transformed, and finally stored in a database or pushed to several other systems for further processing. The other systems can then follow the same cycle—i.e., filter, transform, store, or push to other systems.&lt;/p&gt; &lt;p&gt;In this article, we will build a &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; application that streams and processes data in real-time using &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt;. As we go through the example, you will learn how to apply &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka concepts&lt;/a&gt; such as joins, windows, processors, state stores, punctuators, and interactive queries. By the end of the article, you will have the architecture for a realistic data streaming pipeline in Quarkus.&lt;/p&gt; &lt;h2&gt;The traditional messaging system&lt;/h2&gt; &lt;p&gt;As developers, we are tasked with updating a message-processing system that was originally built using a relational database and a traditional message broker. Here&amp;#8217;s the data flow for the messaging system:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Data from two different systems arrives in two different messaging queues. Each record in one queue has a corresponding record in the other queue. Each record has a unique key.&lt;/li&gt; &lt;li&gt;When a data record arrives in one of the message queues, the system uses the record&amp;#8217;s unique key to determine whether the database already has an entry for that record. If it does not find a record with that unique key, the system inserts the record into the database for processing.&lt;/li&gt; &lt;li&gt;If the same data record arrives in the second queue within a few seconds, the application triggers the same logic. It checks whether a record with the same key is present in the database. If the record is present, the application retrieves the data and processes the two data objects.&lt;/li&gt; &lt;li&gt;If the data record doesn&amp;#8217;t arrive in the second queue within 50 seconds after arriving in the first queue, then another application processes the record in the database.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As you might imagine, this scenario worked well before the advent of data streaming, but it does not work so well today.&lt;/p&gt; &lt;h2&gt;The data streaming pipeline&lt;/h2&gt; &lt;p&gt;Our task is to build a new message system that executes data streaming operations with Kafka. This type of application is capable of processing data in real-time, and it eliminates the need to maintain a database for unprocessed records. Figure 1 illustrates the data flow for the new application:&lt;/p&gt; &lt;div id="attachment_749567" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution.jpg"&gt;&lt;img aria-describedby="caption-attachment-749567" class="wp-image-749567 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-1024x394.jpg" alt="A flow diagram of the data-streaming pipeline's architecture." width="640" height="246" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-1024x394.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-300x115.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/solution-768x295.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749567" class="wp-caption-text"&gt;Figure 1: Architecture of the data streaming pipeline.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;In the next sections, we&amp;#8217;ll go through the process of building a data streaming pipeline with Kafka Streams in Quarkus. You can get the complete source code from the article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus"&gt;GitHub repository&lt;/a&gt;. Before we start coding the architecture, let&amp;#8217;s discuss joins and windows in Kafka Streams.&lt;/p&gt; &lt;h2&gt;Joins and windows in Kafka Streams&lt;/h2&gt; &lt;p&gt;Kafka allows you to &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#joining"&gt;join&lt;/a&gt; records that arrive on two different topics. You are probably familiar with the concept of &lt;i&gt;joins&lt;/i&gt; in a relational database, where the data is static and available in two tables. In Kafka, joins work differently because the data is always streaming.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll look at the types of joins in a moment, but the first thing to note is that joins happen for data collected over a duration of time. Kafka calls this type of collection &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing"&gt;windowing&lt;/a&gt;. Various &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing"&gt;types of windows&lt;/a&gt; are available in Kafka. For our example, we will use a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#windowing-tumbling"&gt;tumbling window&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Inner joins&lt;/h3&gt; &lt;p&gt;Now, let&amp;#8217;s consider how an inner join works. Assume that two separate data streams arrive in two different Kafka topics, which we will call the left and right topics. A record arriving in one topic has another relevant record (with the same key but a different value) that is also arriving in the other topic. The second record arrives after a brief time delay. As shown in Figure 2, we create a Kafka stream for each of the topics.&lt;/p&gt; &lt;div id="attachment_749587" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join.jpg"&gt;&lt;img aria-describedby="caption-attachment-749587" class="wp-image-749587 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-1024x337.jpg" alt="A diagram of an inner join for two topics." width="640" height="211" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-1024x337.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-300x99.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join-768x253.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/inner-join.jpg 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749587" class="wp-caption-text"&gt;Figure 2: Diagram of an inner join.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The inner join on the left and right streams creates a new data stream. When it finds a matching record (with the same key) on both the left and right streams, Kafka emits a new record at time &lt;i&gt;t2&lt;/i&gt; in the new stream. Because the B record did not arrive on the right stream within the specified time window, Kafka Streams won&amp;#8217;t emit a new record for B.&lt;/p&gt; &lt;h3&gt;Outer joins&lt;/h3&gt; &lt;p&gt;Next, let&amp;#8217;s look at how an outer join works. Figure 3 shows the data flow for the outer join in our example:&lt;/p&gt; &lt;div id="attachment_749597" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join.jpg"&gt;&lt;img aria-describedby="caption-attachment-749597" class="wp-image-749597 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-1024x498.jpg" alt="A diagram of an outer join." width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-1024x498.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-300x146.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join-768x374.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/outer-join.jpg 1278w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-749597" class="wp-caption-text"&gt;Figure 3: Diagram of an outer join.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If we don&amp;#8217;t use the &amp;#8220;group by&amp;#8221; clause when we join two streams in Kafka Streams, then the join operation will emit three records. Streams in Kafka do not wait for the entire window; instead, they start emitting records whenever the condition for an outer join is true. So, when Record A on the left stream arrives at time &lt;i&gt;t1&lt;/i&gt;, the join operation immediately emits a new record. At time t2, the &lt;code&gt;outerjoin&lt;/code&gt; Kafka stream receives data from the right stream. The join operation immediately emits another record with the values from both the left and right records.&lt;/p&gt; &lt;p&gt;You would see different outputs if you used the &lt;code&gt;groupBy&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; functions on these Kafka streams. In that case, the streams would wait for the window to complete the duration, perform the join, and then emit the data, as previously shown in Figure 3.&lt;/p&gt; &lt;p&gt;Understanding how inner and outer joins work in Kafka Streams helps us find the best way to implement the data flow that we want. In this case, it is clear that we need to perform an outer join. This type of join allows us to retrieve records that appear in both the left and right topics, as well as records that appear in only one of them.&lt;/p&gt; &lt;p&gt;With that background out of the way, let&amp;#8217;s begin building our Kafka-based data streaming pipeline.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: We can use Quarkus extensions for Spring Web and Spring DI (dependency injection) to code in the &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; style using Spring-based annotations.&lt;/p&gt; &lt;h2&gt;Step 1: Perform the outer join&lt;/h2&gt; &lt;p&gt;To perform the outer join, we first create a class called &lt;code&gt;KafkaStreaming&lt;/code&gt;, then add the function &lt;code&gt;startStreamStreamOuterJoin()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;@RestController public class KafkaStreaming { private KafkaStreams streamsOuterJoin; private final String LEFT_STREAM_TOPIC = "left-stream-topic"; private final String RIGHT_STREAM_TOPIC = "right-stream-topic"; private final String OUTER_JOIN_STREAM_OUT_TOPIC = "stream-stream-outerjoin"; private final String PROCESSED_STREAM_OUT_TOPIC = "processed-topic"; private final String KAFKA_APP_ID = "outerjoin"; private final String KAFKA_SERVER_NAME = "localhost:9092"; @RequestMapping("/startstream/") public void startStreamStreamOuterJoin() { Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, KAFKA_APP_ID); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, KAFKA_SERVER_NAME); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); final StreamsBuilder builder = new StreamsBuilder(); KStream&amp;#60;String, String&amp;#62; leftSource = builder.stream(LEFT_STREAM_TOPIC); KStream&amp;#60;String, String&amp;#62; rightSource = builder.stream(RIGHT_STREAM_TOPIC); // TODO 1 - Add state store // do the outer join // change the value to be a mix of both streams value // have a moving window of 5 seconds // output the last value received for a specific key during the window // push the data to OUTER_JOIN_STREAM_OUT_TOPIC topic leftSource.outerJoin(rightSource, (leftValue, rightValue) -&amp;#62; "left=" + leftValue + ", right=" + rightValue, JoinWindows.of(Duration.ofSeconds(5))) .groupByKey() .reduce(((key, lastValue) -&amp;#62; lastValue)) .toStream() .to(OUTER_JOIN_STREAM_OUT_TOPIC); // build the streams topology final Topology topology = builder.build(); // TODO - 2: Add processor code later streamsOuterJoin = new KafkaStreams(topology, props); streamsOuterJoin.start(); } } &lt;/pre&gt; &lt;p&gt;When we do a join, we create a new value that combines the data in the left and right topics. If any record with a key is missing in the left or right topic, then the new value will have the string &lt;code&gt;null&lt;/code&gt; as the value for the missing record. Also, the Kafka Stream &lt;code&gt;reduce&lt;/code&gt; function returns the last-aggregated value for all of the keys.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;TODO 1 - Add state store&lt;/code&gt; and &lt;code&gt;TODO - 2:  Add processor code later&lt;/code&gt; comments are placeholders for code that we will add in the upcoming sections.&lt;/p&gt; &lt;h3&gt;The data flow so far&lt;/h3&gt; &lt;p&gt;Figure 4 illustrates the following data flow:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When a record with key A and value V1 comes into the left stream at time t1, Kafka Streams applies an outer join operation. At this point, the application creates a new record with key A and the value &lt;em&gt;left=V1, right=null&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;When a record with key A and value V2 arrives in the right topic, Kafka Streams again applies an outer join operation. This creates a new record with key A and the value &lt;em&gt;left=V1, right=V2&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;When the &lt;code&gt;reduce&lt;/code&gt; function is evaluated at the end of the duration window, the Kafka Streams API emits the last value that was computed, per the unique record key. In this case, it emits a record with key A and a value of &lt;em&gt;left=V1, right=V2&lt;/em&gt; into the new stream.&lt;/li&gt; &lt;li&gt;The new stream pushes the record to the &lt;code&gt;outerjoin&lt;/code&gt; topic.&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_786687" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-786687" class="wp-image-786687 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-1024x385.jpg" alt="A diagram of the data streaming pipeline." width="640" height="241" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-1024x385.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-300x113.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/join-to-topic-1-768x289.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-786687" class="wp-caption-text"&gt;Figure 4: The data streaming pipeline so far.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, we will add the state store and processor code.&lt;/p&gt; &lt;h2&gt;Step 2: Add the Kafka Streams processor&lt;/h2&gt; &lt;p&gt;We need to process the records that are being pushed to the &lt;code&gt;outerjoin&lt;/code&gt; topic by the outer join operation. Kafka Streams provides a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html"&gt;Processor API&lt;/a&gt; that we can use to write custom logic for record processing. To start, we define a custom processor, &lt;code&gt;DataProcessor&lt;/code&gt;, and add it to the streams topology in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt;public class DataProcessor implements Processor&amp;#60;String, String&amp;#62;{ private ProcessorContext context; @Override public void init(ProcessorContext context) { this.context = context; } @Override public void process(String key, String value) { if(value.contains("null")) { // TODO 3: - let's process later } else { processRecord(key, value); //forward the processed data to processed-topic topic context.forward(key, value); } context.commit(); } @Override public void close() { } private void processRecord (String key, String value) { // your own custom logic. I just print System.out.println("==== Record Processed ==== key: "+key+" and value: "+value); } } &lt;/pre&gt; &lt;p&gt;The record is processed, and if the value does not contain a &lt;code&gt;null&lt;/code&gt; string, it is forwarded to the &lt;i&gt;sink&lt;/i&gt; topic (that is, the &lt;code&gt;processed-topic&lt;/code&gt; topic). In the bolded parts of the &lt;code&gt;KafkaStreaming&lt;/code&gt; class below, we wire the topology to define the source topic (i.e., the &lt;code&gt;outerjoin&lt;/code&gt; topic), add the processor, and finally add a sink (i.e., the &lt;code&gt;processed-topic&lt;/code&gt; topic). Once it&amp;#8217;s done, we can add this piece of code to the &lt;code&gt;TODO - 2: Add processor code later&lt;/code&gt; section of the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt;// add another stream that reads data from OUTER_JOIN_STREAM_OUT_TOPIC topic topology.addSource("Source", OUTER_JOIN_STREAM_OUT_TOPIC); // add a processor to the stream so that each record is processed topology.addProcessor("StateProcessor", new ProcessorSupplier&amp;#60;String, String&amp;#62;() { public Processor&amp;#60;String, String&amp;#62; get() { return new DataProcessor(); }}, "Source"); topology.addSink("Sink", PROCESSED_STREAM_OUT_TOPIC, "StateProcessor"); &lt;/pre&gt; &lt;p&gt;Note that all we do is to define the source topic (the &lt;code&gt;outerjoin&lt;/code&gt; topic), add an instance of our custom processor class, and then add the sink topic (the &lt;code&gt;processed-topic&lt;/code&gt; topic). The &lt;code&gt;context.forward()&lt;/code&gt; method in the custom processor sends the record to the sink topic.&lt;/p&gt; &lt;p&gt;Figure 5 shows the architecture that we have built so far.&lt;/p&gt; &lt;div id="attachment_786707" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor.jpg"&gt;&lt;img aria-describedby="caption-attachment-786707" class="wp-image-786707 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-1024x381.jpg" alt="A diagram of the architecture in progress." width="640" height="238" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-1024x381.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-300x112.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/processor-768x286.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786707" class="wp-caption-text"&gt;Figure 5: The architecture with the Kafka Streams processor added.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Step 3: Add the punctuator and StateStore&lt;/h2&gt; &lt;p&gt;If you looked closely at the &lt;code&gt;DataProcessor&lt;/code&gt; class, you probably noticed that we are only processing records that have both of the required (left-stream and right-stream) key values. We also need to process records that have just one of the values, but we want to introduce a delay before processing these records. In some cases, the other value will arrive in a later time window, and we don&amp;#8217;t want to process the records prematurely.&lt;/p&gt; &lt;h3&gt;State store&lt;/h3&gt; &lt;p&gt;In order to delay processing, we need to hold incoming records in a store of some kind, rather than an external database. Kafka Streams lets us store data in a &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html#state-stores"&gt;state store&lt;/a&gt;. We can use this type of store to hold recently received input records, track rolling aggregates, de-duplicate input records, and more.&lt;/p&gt; &lt;h3&gt;Punctuators&lt;/h3&gt; &lt;p&gt;Once we start holding records that have a missing value from either topic in a state store, we can use &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/processor-api.html#defining-a-stream-processor"&gt;punctuators&lt;/a&gt; to process them. As an example, we could add a &lt;code&gt;punctuator&lt;/code&gt; function to a &lt;code&gt;processorcontext.schedule()&lt;/code&gt; method. We can set the schedule to call the &lt;code&gt;punctuate()&lt;/code&gt; method.&lt;/p&gt; &lt;h3&gt;Add the state store&lt;/h3&gt; &lt;p&gt;Adding the following code to the &lt;code&gt;KafkaStreaming&lt;/code&gt; class adds a state store. Place this code where you see the &lt;code&gt;TODO 1 - Add state store&lt;/code&gt; comment in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; // build the state store that will eventually store all unprocessed items Map&amp;#60;String, String&amp;#62; changelogConfig = newHashMap&amp;#60;&amp;#62;(); StoreBuilder&amp;#60;KeyValueStore&amp;#60;String, String&amp;#62;&amp;#62; stateStore = Stores.keyValueStoreBuilder( Stores.persistentKeyValueStore(STORE_NAME), Serdes.String(), Serdes.String()) .withLoggingEnabled(changelogConfig); ..... ..... ..... ..... // add the state store in the topology builder topology.addStateStore(stateStore, "StateProcessor"); &lt;/pre&gt; &lt;p&gt;We have defined a state store that stores the key and value as a string. We&amp;#8217;ve also enabled logging, which is useful if the application dies and restarts. In that case, the state store won&amp;#8217;t lose data.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll modify the processor&amp;#8217;s &lt;code&gt;process()&lt;/code&gt; to put records with a missing value from either topic in the state store for later processing. Place the following code where you see the comment &lt;code&gt;TODO 3 - let's process later&lt;/code&gt; in the &lt;code&gt;KafkaStreaming&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; if(value.contains("null")) { if (kvStore.get(key) != null) { // this means that the other value arrived first // you have both the values now and can process the record String newvalue = value.concat(" ").concat(kvStore.get(key)); process(key, newvalue); // remove the entry from the statestore (if any left or right record came first as an event) kvStore.delete(key); context.forward(key, newvalue); } else { // add to state store as either left or right data is missing System.out.println("Incomplete value: "+value+" detected. Putting into statestore for later processing"); kvStore.put(key, value); } } &lt;/pre&gt; &lt;h3&gt;Add the punctuator&lt;/h3&gt; &lt;p&gt;Next, we add the punctuator to the custom processor we&amp;#8217;ve just created. For this, we update the &lt;code&gt;DataProcessor&lt;/code&gt;&amp;#8216;s &lt;code&gt;init()&lt;/code&gt; method to the following:&lt;/p&gt; &lt;pre&gt; private KeyValueStore&amp;#60;String, String&amp;#62; kvStore; @Override public void init(ProcessorContext context) { this.context = context; kvStore = (KeyValueStore) context.getStateStore(STORE_NAME); // schedule a punctuate() method every 50 seconds based on stream-time this.context.schedule(Duration.ofSeconds(50), PunctuationType.WALL_CLOCK_TIME, new Punctuator(){ @Override public void punctuate(long timestamp) { System.out.println("Scheduled punctuator called at "+timestamp); KeyValueIterator&amp;#60;String, String&amp;#62; iter = kvStore.all(); while (iter.hasNext()) { KeyValue&amp;#60;String, String&amp;#62; entry = iter.next(); System.out.println(" Processed key: "+entry.key+" and value: "+entry.value+" and sending to processed-topic topic"); context.forward(entry.key, entry.value.toString()); kvStore.put(entry.key, null); } iter.close(); // commit the current processing progress context.commit(); } } ); } &lt;/pre&gt; &lt;p&gt;We&amp;#8217;ve set the punctuate logic to be invoked every 50 seconds. The code retrieves entries in the state store and processes them. The &lt;code&gt;forward()&lt;/code&gt; function then sends the processed record to the &lt;code&gt;processed-topic&lt;/code&gt; topic. Lastly, we delete the record from the state store.&lt;/p&gt; &lt;p&gt;Figure 6 shows the complete data streaming architecture:&lt;/p&gt; &lt;div id="attachment_786717" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture.jpg"&gt;&lt;img aria-describedby="caption-attachment-786717" class="wp-image-786717 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-1024x390.jpg" alt="A diagram of the complete application with the state store and punctuators added." width="640" height="244" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-1024x390.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-300x114.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/complete-architecture-768x293.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-786717" class="wp-caption-text"&gt;Figure 6: The complete data streaming pipeline.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Interactive queries&lt;/h2&gt; &lt;p&gt;We are finished with the basic data streaming pipeline, but what if we wanted to be able to query the state store? In this case, we could use &lt;a target="_blank" rel="nofollow" href="https://kafka.apache.org/10/documentation/streams/developer-guide/interactive-queries.html"&gt;interactive queries&lt;/a&gt; in the Kafka Streams API to make the application queryable. See the article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus/blob/master/quarkus-kafka-streaming/src/main/java/org/acme/InteractiveQueries.java"&gt;GitHub repository&lt;/a&gt; for more about interactive queries in Kafka Streams.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;You can use the streaming pipeline that we developed in this article to do any of the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Process records in real-time.&lt;/li&gt; &lt;li&gt;Store data without depending on a database or cache.&lt;/li&gt; &lt;li&gt;Build a modern, &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hope the example application and instructions will help you with building and processing data streaming pipelines. You can get the source code for the example application from this article&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/kgshukla/data-streaming-kafka-quarkus"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#38;linkname=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F28%2Fbuild-a-data-streaming-pipeline-using-kafka-streams-and-quarkus%2F&amp;#038;title=Build%20a%20data%20streaming%20pipeline%20using%20Kafka%20Streams%20and%20Quarkus" data-a2a-url="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/" data-a2a-title="Build a data streaming pipeline using Kafka Streams and Quarkus"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/"&gt;Build a data streaming pipeline using Kafka Streams and Quarkus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6-PYLEG5zIc" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In typical data warehousing systems, data is first accumulated and then processed. But with the advent of new technologies, it is now possible to process data as and when it arrives. We call this real-time data processing. In real-time processing, data streams through pipelines; i.e., moving from one system to another. Data gets generated from [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/"&gt;Build a data streaming pipeline using Kafka Streams and Quarkus&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">749517</post-id><dc:creator>Kapil Shukla</dc:creator><dc:date>2020-09-28T07:00:22Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/28/build-a-data-streaming-pipeline-using-kafka-streams-and-quarkus/</feedburner:origLink></entry><entry><title>How to setup OpenShift Container Platform 4.5 on your local machine in minutes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/zFfVGIyo8UA/how-to-setup-openshift-container-platform-45.html" /><category term="cloud" scheme="searchisko:content:tags" /><category term="CodeReadyContainers" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-how_to_setup_openshift_container_platform_4_5_on_your_local_machine_in_minutes</id><updated>2020-09-28T05:00:05Z</updated><published>2020-09-28T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-ezsrnZBkM34/X2IOwlzFutI/AAAAAAAAxi0/sbIfjQl1aH8_W9fJW5fccdT26o2ka92bQCNcBGAsYHQ/s1600/kevin-ku-w7ZyuGYNpRQ-unsplash.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img alt="CodeReady Containers" border="0" data-original-height="1200" data-original-width="1600" height="240" src="https://1.bp.blogspot.com/-ezsrnZBkM34/X2IOwlzFutI/AAAAAAAAxi0/sbIfjQl1aH8_W9fJW5fccdT26o2ka92bQCNcBGAsYHQ/s320/kevin-ku-w7ZyuGYNpRQ-unsplash.jpg" title="" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform?&lt;br /&gt;&lt;br /&gt;Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.5?&lt;br /&gt;&lt;br /&gt;Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud native development and automated rolling deployments. Since I started pulling together ways to easily experience this with OpenShift Container Platform, back with&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;version 3.3&lt;/a&gt;&amp;nbsp;believe it or not, we've come a long ways.&lt;br /&gt;&lt;br /&gt;The idea was to make this as streamlined of an experience as possible by using the same&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;CodeReady Containers Easy Install project&lt;/a&gt;. Let's take a look at what this looks like.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;The first focus was to have this work for Unix based operating systems using a single installation script. The secondary wish is to provide a windows based installation script.&lt;br /&gt;&lt;br /&gt;&lt;h3 data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}"&gt;Linux or Mac installation&lt;/h3&gt;This installation requires the following (all freely available):&lt;br /&gt;&lt;br /&gt;&lt;pre class="code highlight" lang="plaintext"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&lt;span class="line" data-keep-original-tag="false" id="LC1" lang="plaintext"&gt;1. HyperKit for OSX, Hyper-V for Windows, or Libvirt for Linux&lt;/span&gt;&lt;br /&gt;&lt;span class="line" data-keep-original-tag="false" id="LC2" lang="plaintext"&gt;2. Code Ready Containers (OCP 4.5)&lt;/span&gt;&lt;br /&gt;&lt;span class="line" data-keep-original-tag="false" id="LC3" lang="plaintext"&gt;3. OpenShift Client (oc) v4.5&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/pre&gt;&lt;br /&gt;First you need to ensure your virtualization tooling is installed for your platform, just search online for how to do that or your specific platform. Second you need to download the CodeReady Containers. Finally, you need the OpenShift client. Normally you'd expect to have to track these last two down but we've made this all easy by just including checks during the installation. If you have something installed, it checks the version, if good then it moves on with next steps. If anything is missing or wrong version, the installation stops and notifies you where to find that component for your platform (including URL).&lt;br /&gt;&lt;br /&gt;Let's get started by downloading the&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/ocp-install-demo/-/archive/master/ocp-install-demo-master.zip" target="_blank"&gt;CodeReady Containers Easy Install&lt;/a&gt; project and unzipping in some directory. This gives you a file called&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;ocp-install-demo-master.zip&lt;/span&gt;,&lt;/span&gt;just unzip and run the&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;init.sh&lt;/span&gt;&lt;/span&gt;&amp;nbsp;as follows:&lt;br /&gt;&lt;br /&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $ ./init.sh&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Follow the instructions as each of the dependencies is checked and you're provided with pointers to getting the versions you need for your platform.&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Note: Each CodeReady Container download is tied to an embedded secret. This secret you need to download (link will be provided) as a file and you'll be asked to point to that secret to start your container platform.&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;Once you've gotten all the dependencies sorted out, the install runs like this:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-Zam6j6uD34w/X2H3oCplT_I/AAAAAAAAxhE/0nu75cf7gFgUEkHRB2zMByyDJb-zo9tTQCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B13.31.31.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="844" data-original-width="792" height="640" src="https://1.bp.blogspot.com/-Zam6j6uD34w/X2H3oCplT_I/AAAAAAAAxhE/0nu75cf7gFgUEkHRB2zMByyDJb-zo9tTQCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B13.31.31.png" width="600" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;A little ASCII art and then it's checking for my platforms virtualization (Hyperkit), then looking for the OpenShift client version 4.5 (oc client), then running a setup (crc setup).&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-1Mhlckwprew/X2H35HT9KSI/AAAAAAAAxhM/HSuSwsFWuKwlNjC_gSBhEJYAYk30h6wvgCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B13.32.42.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="406" data-original-width="813" height="316" src="https://1.bp.blogspot.com/-1Mhlckwprew/X2H35HT9KSI/AAAAAAAAxhM/HSuSwsFWuKwlNjC_gSBhEJYAYk30h6wvgCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B13.32.42.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;The next steps are providing the pull-secret-file, you can set this in the variables at the top of the installation script. Now the moment of truth, the CodeReady Containers cluster starts, which takes some time depending on your network (crc start). With a good networks it's about a five minute wait.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-oOe-GagsJAI/X2IBlVKuYDI/AAAAAAAAxhY/XULsOy81nYsukh9gv4CJLqZchcc2DZZ4QCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.07.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="578" data-original-width="978" height="378" src="https://1.bp.blogspot.com/-oOe-GagsJAI/X2IBlVKuYDI/AAAAAAAAxhY/XULsOy81nYsukh9gv4CJLqZchcc2DZZ4QCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.07.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;This is the logging you'll see as the OpenShift cluster starts on your local machine. The warning is normal, just some of the features have been trimmed to speed up deployment.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-9vrBZLMiS64/X2IButB2LYI/AAAAAAAAxhg/7tqEa0FaATgjB9cK6t0UHixRBoHooCfgwCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.46.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="259" data-original-width="780" height="212" src="https://1.bp.blogspot.com/-9vrBZLMiS64/X2IButB2LYI/AAAAAAAAxhg/7tqEa0FaATgjB9cK6t0UHixRBoHooCfgwCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.46.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-6CAagw6bs7c/X2IBuranlVI/AAAAAAAAxhc/1pdEOE9pqvswXZ2FGAGOs48QJ3Gqo4A_ACNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.13.57.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="461" data-original-width="553" height="531" src="https://1.bp.blogspot.com/-6CAagw6bs7c/X2IBuranlVI/AAAAAAAAxhc/1pdEOE9pqvswXZ2FGAGOs48QJ3Gqo4A_ACNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.13.57.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;At the end we'll retrieve the admin password for logging in to the cluster's console, pick up the host URL, test the deployment by logging in with our client (oc login), and finally you're given all the details in a nice box. You have the option to stop, start it again, or delete the OpenShift Container Platform cluster as shown in the dialog.&lt;br /&gt;&lt;br /&gt;Next open the web console using URL and login 'kubeadmin' with the corresponding password. In our case it's the URL:&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace; font-size: x-small;"&gt;https://console-openshift-console.apps-crc.testing&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/--B9bC4IjKGc/X2ICGZjji4I/AAAAAAAAxhw/7og6eJ6RKIsu2hRqHHQsDfrtt0fckNOkACNcBGAsYHQ/s1600/ocp-login.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="804" data-original-width="1600" height="320" src="https://1.bp.blogspot.com/--B9bC4IjKGc/X2ICGZjji4I/AAAAAAAAxhw/7og6eJ6RKIsu2hRqHHQsDfrtt0fckNOkACNcBGAsYHQ/s640/ocp-login.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;Log in with user:&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-size: x-small;"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;kubeadmin&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;Password in our case:&amp;nbsp;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace; font-size: x-small;"&gt;duduw-yPT9Z-hsUpq-f3pre&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;That opens the main dashboard:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-guFcKoTIsR4/X2ICTPQJdNI/AAAAAAAAxh0/eK5qAyCYrAEbc3lZhZysU2MMs5Xh_eEOwCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.17.04.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="898" data-original-width="1534" height="374" src="https://1.bp.blogspot.com/-guFcKoTIsR4/X2ICTPQJdNI/AAAAAAAAxh0/eK5qAyCYrAEbc3lZhZysU2MMs5Xh_eEOwCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.17.04.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;Verify the version you are running by clicking on the top right question mark and then &lt;i&gt;About&lt;/i&gt;&amp;nbsp;option:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-1Y0cF0HHgJQ/X2ICqMAgeFI/AAAAAAAAxiE/D76GzK_OxmYAcb6xDdYIoXi-OhYaNUKwgCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.11.54.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="900" data-original-width="1533" height="374" src="https://1.bp.blogspot.com/-1Y0cF0HHgJQ/X2ICqMAgeFI/AAAAAAAAxiE/D76GzK_OxmYAcb6xDdYIoXi-OhYaNUKwgCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.11.54.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Close the version window by clicking on the X. As we are interested in developing using the tooling and container images provided by CodeReady Containers, let's change the view from &lt;i&gt;Administrator &lt;/i&gt;to &lt;i&gt;Developer &lt;/i&gt;in the left top menu selecting &lt;i&gt;Topology &lt;/i&gt;and then via &lt;i&gt;Project&lt;/i&gt;&amp;nbsp;drop down menu at the top choose &lt;i&gt;Default&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-cZNIzNMZboE/X2IDYdKmFjI/AAAAAAAAxic/Q-fD4xSoAqk2TtfzLwY3uY31FnCGkPh1ACNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.20.53.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="899" data-original-width="1537" height="374" src="https://1.bp.blogspot.com/-cZNIzNMZboE/X2IDYdKmFjI/AAAAAAAAxic/Q-fD4xSoAqk2TtfzLwY3uY31FnCGkPh1ACNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.20.53.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;br /&gt;You can browse the offerings in the provided container catalog by selecting &lt;i&gt;From Catalog&lt;/i&gt;&amp;nbsp;and then for example, &lt;i&gt;Middleware &lt;/i&gt;to view the offerings available:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-0F8QRn_Ustw/X2IDxZDFg0I/AAAAAAAAxio/Ewhn9ClT6Cc3J73Lb2Qc3sPimreZFYKpQCNcBGAsYHQ/s1600/Screenshot%2B2020-09-16%2Bat%2B14.22.57.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="826" data-original-width="1536" height="344" src="https://1.bp.blogspot.com/-0F8QRn_Ustw/X2IDxZDFg0I/AAAAAAAAxio/Ewhn9ClT6Cc3J73Lb2Qc3sPimreZFYKpQCNcBGAsYHQ/s640/Screenshot%2B2020-09-16%2Bat%2B14.22.57.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Looking to get started with an example usage, try the &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rhpam-install-demo" target="_blank"&gt;Red Hat Process Automation Manager&lt;/a&gt; or &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rhdm-install-demo" target="_blank"&gt;Red Hat Decision Manager &lt;/a&gt;examples that leverage the provided developer catalog container images. You can also explore how an existing project is setup using one of the developer catalog container images with a &lt;a href="https://gitlab.com/redhatdemocentral/rhcs-rewards-demo" target="_blank"&gt;human resources employee rewards project&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;This concludes the installation and tour of an OpenShift Container Platform on our local machine using CodeReady Containers.&lt;br /&gt;&lt;br /&gt;&lt;h3 data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}"&gt;What about Windows?&lt;/h3&gt;If you are a sharp observer, you'll notice there is a file called&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo/blob/master/init.bat&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;&lt;span data-keep-original-tag="false" data-original-attrs="{&amp;quot;style&amp;quot;:&amp;quot;&amp;quot;}" style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;init.bat&lt;/span&gt;&lt;/a&gt;&amp;nbsp;for windows platforms to install with. The problem is I've not been able to test this yet on a windows machine, so I'd love to call out to the readers out there that might have some time to contribute to test this script and help us complete the installation. You'll notice a few TODO's marked in the scripts code, as they are untested areas in the installation.&lt;br /&gt;&lt;br /&gt;You can&amp;nbsp;&lt;a data-original-attrs="{&amp;quot;data-original-href&amp;quot;:&amp;quot;https://gitlab.com/redhatdemocentral/ocp-install-demo/issues/new?issue%5Bassignee_id%5D=&amp;amp;issue%5Bmilestone_id%5D=&amp;quot;,&amp;quot;target&amp;quot;:&amp;quot;_blank&amp;quot;}" href="https://www.blogger.com/blog/post/edit/3868547292717970492/8029179968560939113#"&gt;raise new issues here&lt;/a&gt;&amp;nbsp;and help us complete the windows based installation and get your name added to the contributors list. We'd be really thankful!&lt;br /&gt;&lt;br /&gt;Stay tuned for more on cloud-native development using other Red Hat technologies on your new OpenShift Container Platform installed locally on your own machine!&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=n_b1jiOlhGY:w73WrTpq09c:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=n_b1jiOlhGY:w73WrTpq09c:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/n_b1jiOlhGY" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/zFfVGIyo8UA" height="1" width="1" alt=""/&gt;</content><summary>Are you looking to develop a few projects on your local machine and push them on to a real OpenShift Container Platform without having to worry about cloud hosting of your container platform? Would you like to do that on one of the newer versions of OpenShift Container Platform such as version 4.5? Look no further as CodeReady Containers puts it all at your fingertips. Experience the joys of cloud...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2020-09-28T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/n_b1jiOlhGY/how-to-setup-openshift-container-platform-45.html</feedburner:origLink></entry><entry><title>Rootless containers with Podman: The basics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qM0ia1DXS7w/" /><category term="Containers" /><category term="Linux" /><category term="Open source" /><category term="Universal Base Images (UBI)" /><category term="oci image" /><category term="Podman" /><category term="rhel 7" /><category term="rhel 8" /><category term="rootless containers" /><category term="rootless podman" /><author><name>Prakhar Sethi</name></author><id>https://developers.redhat.com/blog/?p=748207</id><updated>2020-09-25T07:00:56Z</updated><published>2020-09-25T07:00:56Z</published><content type="html">&lt;p&gt;As a developer, you have probably heard a lot about containers. A &lt;a href="https://developers.redhat.com/topics/containers"&gt;&lt;i&gt;container&lt;/i&gt;&lt;/a&gt; is a unit of software that provides a packaging mechanism that abstracts the code and all of its dependencies to make application builds fast and reliable. An easy way to experiment with containers is with the Pod Manager tool (&lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools"&gt;Podman&lt;/a&gt;), which is a daemonless, open source, &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;-native tool that provides a command-line interface (CLI) similar to the docker container engine.&lt;/p&gt; &lt;p&gt;In this article, I will explain the benefits of using containers and Podman, introduce rootless containers and why they are important, and then show you how to use rootless containers with Podman with an example. Before we dive into the implementation, let&amp;#8217;s review the basics.&lt;/p&gt; &lt;p&gt;&lt;span id="more-748207"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Why containers?&lt;/h2&gt; &lt;p&gt;Using containers isolates your applications from the various computing environments in which they run. They have become increasingly popular because they help developers focus on the application logic and its dependencies, which they bind in a single unit. Operations teams also like containers because they can focus on managing the application, including deployment, without bothering with details such as software versions and configuration.&lt;/p&gt; &lt;p&gt;Containers virtualize at the operating system (OS) level. This makes them lightweight, unlike virtual machines, which virtualize at the hardware level. In a nutshell, here are the advantages of using containers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Low hardware footprint&lt;/li&gt; &lt;li&gt;Environment isolation&lt;/li&gt; &lt;li&gt;Quick deployment&lt;/li&gt; &lt;li&gt;Multiple environment deployments&lt;/li&gt; &lt;li&gt;Reusability&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why Podman?&lt;/h2&gt; &lt;p&gt;Using Podman makes it easy to find, run, build, share, and deploy applications using &lt;a target="_blank" rel="nofollow" href="https://opencontainers.org/"&gt;Open Container Initiative&lt;/a&gt; (OCI)-compatible containers and container images. Podman&amp;#8217;s advantages are as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It is &lt;em&gt;daemonless&lt;/em&gt;; it does not require a daemon, unlike docker.&lt;/li&gt; &lt;li&gt;It lets you control the layers of the container; sometimes, you want a single layer, and sometimes you need 12 layers.&lt;/li&gt; &lt;li&gt;It uses the fork/exec model for containers instead of the client/server model.&lt;/li&gt; &lt;li&gt;It lets you run containers as a non-root user, so you never have to give a user root permission on the host. This obviously differs from the client/server model, where you must open a socket to a privileged daemon running as root to launch a container.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Why rootless containers?&lt;/h2&gt; &lt;p&gt;&lt;i&gt;Rootless containers&lt;/i&gt; are containers that can be created, run, and managed by users without admin rights. Rootless containers have several advantages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;They add a new security layer; even if the container engine, runtime, or orchestrator is compromised, the attacker won&amp;#8217;t gain root privileges on the host.&lt;/li&gt; &lt;li&gt;They allow multiple unprivileged users to run containers on the same machine (this is especially advantageous in &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/blog/podman-paves-road-running-containerized-hpc-applications-exascale-supercomputers"&gt;high-performance computing environments&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;They allow for isolation inside of nested containers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To better understand these advantages, consider traditional resource management and scheduling systems. This type of system should be run by unprivileged users. From a security perspective, fewer privileges are better. With rootless containers, you can run a containerized process as any other process without needing to escalate any user&amp;#8217;s privileges. There is no daemon; Podman creates a child process.&lt;/p&gt; &lt;h2&gt;Example: Using rootless containers&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s get started using rootless containers with Podman. We&amp;#8217;ll start with the basic setup and configuration.&lt;/p&gt; &lt;h3&gt;System requirements&lt;/h3&gt; &lt;p&gt;We will need &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 7.7 or greater for this implementation. Assuming you have that, we can begin configuring the example.&lt;/p&gt; &lt;h3&gt;Configuration&lt;/h3&gt; &lt;p&gt;First, install &lt;code&gt;slirp4netns&lt;/code&gt; and Podman on your machine by entering the following command:&lt;/p&gt; &lt;pre&gt;$ yum install slirp4netns podman -y &lt;/pre&gt; &lt;p&gt;We will use &lt;code&gt;slirp4netns&lt;/code&gt; to connect a network namespace to the internet in a completely rootless (or unprivileged) way.&lt;/p&gt; &lt;p&gt;When the installation is done, increase the number of user namespaces. Use the following commands :&lt;/p&gt; &lt;pre&gt;$ echo “user.max_user_namespaces=28633” &amp;#62; /etc/sysctl.d/userns.conf $ sysctl -p /etc/sysctl.d/userns.conf &lt;/pre&gt; &lt;p&gt;Next, create a new user account and name it. In this case, my user account is named Red Hat:&lt;/p&gt; &lt;pre&gt;$ useradd -c “Red Hat” redhat &lt;/pre&gt; &lt;p&gt;Use the following command to set the password for the new account (note that you must insert your own password):&lt;/p&gt; &lt;pre&gt;$ passwd redhat &lt;/pre&gt; &lt;p&gt;This user is now automatically configured to be able to use a rootless instance of Podman.&lt;/p&gt; &lt;h3&gt;Connect to the user&lt;/h3&gt; &lt;p&gt;Now, try running a Podman command as the user you&amp;#8217;ve just created.  Do not use &lt;code&gt;su -&lt;/code&gt; because that command doesn&amp;#8217;t set the correct environment variables. Instead, you can use any other command to connect to that user. Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;$ ssh redhat@localhost &lt;/pre&gt; &lt;h3&gt;Pull a Red Hat Enterprise Linux image&lt;/h3&gt; &lt;p&gt;After logging in, try pulling a RHEL image using the &lt;code&gt;podman&lt;/code&gt; command (note that &lt;code&gt;ubi&lt;/code&gt; stands for &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Image&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;$ podman pull ubi7/ubi &lt;/pre&gt; &lt;p&gt;If you want more information about the image, run this command:&lt;/p&gt; &lt;pre&gt;$ podman run ubi7/ubi cat /etc/os-release &lt;/pre&gt; &lt;p&gt;To check the images that resulted from the above command, along with any other images on your system, run the command:&lt;/p&gt; &lt;pre&gt;$ podman images &lt;/pre&gt; &lt;p&gt;It is also possible for a rootless user to create a container from these images, but I&amp;#8217;ll save that for another article.&lt;/p&gt; &lt;h3&gt;Check the rootless configuration&lt;/h3&gt; &lt;p&gt;Finally, verify whether your rootless configuration is properly set up. Run the following command to show how the UIDs are assigned to the user namespace:&lt;/p&gt; &lt;pre&gt;$ podman unshare cat /proc/self/uid_map &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article demonstrated how to set up rootless containers with Podman. Here are some tips for working with rootless containers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;As a non-root container user, container images are stored under your home directory (for instance, &lt;code&gt;$HOME/.local/share/containers/storage&lt;/code&gt;), instead of &lt;code&gt;/var/lib/containers&lt;/code&gt;. This directory scheme ensures that you have enough storage for your home directory.&lt;/li&gt; &lt;li&gt;Users running rootless containers are given special permission to run on the host system using a range of user and group IDs. Otherwise, they have no root privileges to the operating system on the host.&lt;/li&gt; &lt;li&gt;A container running as root in a rootless account can turn on privileged features within its own namespace. But that doesn&amp;#8217;t provide any special privileges to access protected features on the host (beyond having extra UIDs and GIDs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Learn more about &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/index#set_up_for_rootless_containers"&gt;setting up rootless containers with Podman here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#38;linkname=Rootless%20containers%20with%20Podman%3A%20The%20basics" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F25%2Frootless-containers-with-podman-the-basics%2F&amp;#038;title=Rootless%20containers%20with%20Podman%3A%20The%20basics" data-a2a-url="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/" data-a2a-title="Rootless containers with Podman: The basics"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/"&gt;Rootless containers with Podman: The basics&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qM0ia1DXS7w" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;As a developer, you have probably heard a lot about containers. A container is a unit of software that provides a packaging mechanism that abstracts the code and all of its dependencies to make application builds fast and reliable. An easy way to experiment with containers is with the Pod Manager tool (Podman), which is [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/"&gt;Rootless containers with Podman: The basics&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">3</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">748207</post-id><dc:creator>Prakhar Sethi</dc:creator><dc:date>2020-09-25T07:00:56Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics/</feedburner:origLink></entry><entry><title>New C++ features in GCC 10</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JYKdxbvy15s/" /><category term="C++" /><category term="Linux" /><category term="Open source" /><category term="Programming Languages" /><category term="C++20" /><category term="GCC 10" /><category term="rhel 7" /><category term="rhel 8" /><author><name>Marek Polacek</name></author><id>https://developers.redhat.com/blog/?p=749897</id><updated>2020-09-24T07:00:32Z</updated><published>2020-09-24T07:00:32Z</published><content type="html">&lt;p&gt;The GNU Compiler Collection (GCC) 10.1 was released in May 2020. Like every other GCC release, this version brought many &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/gcc-10/changes.html"&gt;additions, improvements, bug fixes, and new features&lt;/a&gt;. Fedora 32 already ships GCC 10 as the system compiler, but it&amp;#8217;s also possible to try GCC 10 on other platforms (see &lt;a target="_blank" rel="nofollow" href="https://godbolt.org/"&gt;godbolt.org&lt;/a&gt;, for example). &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) users will get GCC 10 in the Red Hat Developer Toolset (RHEL 7), or the Red Hat GCC Toolset (RHEL 8).&lt;/p&gt; &lt;p&gt;This article focuses on the part of the GCC compiler on which I spend most of my time: The C++ front end. My goal is to present new features that might be of interest to C++ application programmers. Note that I do not discuss developments in the C++ language itself, although some language updates overlap with compiler updates. I also do not discuss changes in the standard C++ library that comes with GCC 10.&lt;/p&gt; &lt;p&gt;We implemented many C++20 proposals in GCC 10. For the sake of brevity, I won&amp;#8217;t describe them in great detail. The default dialect in GCC 10 is &lt;code&gt;-std=gnu++14&lt;/code&gt;; to enable C++20 features, use the &lt;code&gt;-std=c++20&lt;/code&gt; or &lt;code&gt;-std=gnu++20&lt;/code&gt; command-line option. (Note that the latter option allows GNU extensions.)&lt;/p&gt; &lt;h2&gt;C++ concepts&lt;/h2&gt; &lt;p&gt;While previous versions of GCC (GCC 6 was the first) had initial implementations of &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/p0734r0.pdf"&gt;C++ concepts&lt;/a&gt;, GCC 10 updated concepts to conform to the C++20 specification. This update also improved compile times. Subsequent patches have improved concepts-related diagnostics.&lt;/p&gt; &lt;p&gt;In C++ template parameters, &lt;code&gt;typename&lt;/code&gt; means any type. But most templates must be constrained in some way; as an example, you want to only accept types that have certain properties, not just any type. Failing to use the correct type often results in awful and verbose error messages. In C++20, you can constrain a type by using a &lt;em&gt;concept&lt;/em&gt;, which is a compile-time predicate that specifies the set of operations that can be applied to the type. Using GCC 10, you can define your own concept (or use one defined in &lt;code&gt;&amp;#60;concepts&amp;#62;&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;#include &amp;#60;type_traits&amp;#62; // Require that T be an integral type. template&amp;#60;typename T&amp;#62; concept C = std::is_integral_v&amp;#60;T&amp;#62;; &lt;/pre&gt; &lt;p&gt;And then use it like this:&lt;/p&gt; &lt;pre&gt;template&amp;#60;C T&amp;#62; void f(T) { } void g () { f (1); // OK f (1.2); // error: use of function with unsatisfied constraints } &lt;/pre&gt; &lt;p&gt;Starting with GCC 10, the C++ compiler also supports &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1141r2.html"&gt;constrained auto&lt;/a&gt;. Using our concept above, you can now write:&lt;/p&gt; &lt;pre&gt;int fn1 (); double fn2 (); void h () { C auto x1 = fn1 (); // OK C auto x2 = fn2 (); // error: deduced initializer does not satisfy placeholder constraints }&lt;/pre&gt; &lt;h2&gt;Coroutines&lt;/h2&gt; &lt;p&gt;GCC 10 supports &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0912r5.html"&gt;stackless functions&lt;/a&gt; that can be suspended and resumed later without losing their state. This feature lets us execute sequential code asynchronously. It requires the &lt;code&gt;-fcoroutines&lt;/code&gt; command-line option.&lt;/p&gt; &lt;h3&gt;Unevaluated inline-assembly in &lt;code&gt;constexpr&lt;/code&gt; functions&lt;/h3&gt; &lt;p&gt;Code like this now compiles:&lt;/p&gt; &lt;pre&gt;constexpr int foo (int a, int b) { if (std::is_constant_evaluated ()) return a + b; // Not in a constexpr context. asm ("/* assembly */"); return a; }&lt;/pre&gt; &lt;p&gt;See &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1668r1.html"&gt;the proposal&lt;/a&gt; for details.&lt;/p&gt; &lt;h3&gt;Comma expression in array subscript expressions&lt;/h3&gt; &lt;p&gt;This type of expression is now &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1161r3.html"&gt;deprecated&lt;/a&gt;, so GCC 10 warns for code like this:&lt;/p&gt; &lt;pre&gt;int f (int arr[], int a, int b) { return arr[a, b]; } &lt;/pre&gt; &lt;p&gt;Only a top-level comma is deprecated, however, so &lt;code&gt;arr[(a, b)]&lt;/code&gt; compiles without a warning.&lt;/p&gt; &lt;h3&gt;Structured bindings&lt;/h3&gt; &lt;p&gt;GCC 10 improves and &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1091r3.html"&gt;extends&lt;/a&gt; structured bindings. For instance, it&amp;#8217;s now possible to mark them &lt;code&gt;static&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;struct S { int a, b, c; } s; static auto [ x, y, z ] = s; &lt;/pre&gt; &lt;p&gt;This example doesn&amp;#8217;t compile with GCC 9, but it compiles with GCC 10.&lt;/p&gt; &lt;h2&gt;The &lt;code&gt;constinit&lt;/code&gt; keyword&lt;/h2&gt; &lt;p&gt;GCC 10 uses the &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1143r2.html"&gt;C++20 specifier&lt;/a&gt; &lt;code&gt;constinit&lt;/code&gt; to ensure that a (static storage duration) variable is initialized by a constant initializer.&lt;/p&gt; &lt;p&gt;This might alleviate problems with the &lt;em&gt;static initialization order fiasco&lt;/em&gt;. However, the variable is not constant: It is possible to modify it after initialization has taken place. Consider:&lt;/p&gt; &lt;pre&gt;constexpr int fn1 () { return 42; } int fn2 () { return -1; } constinit int i1 = fn1 (); // OK constinit int i2 = fn2 (); // error: constinit variable does not have a constant initializer &lt;/pre&gt; &lt;p&gt;GCC doesn&amp;#8217;t support Clang&amp;#8217;s &lt;code&gt;require_constant_initialization&lt;/code&gt; attribute, so you can use &lt;code&gt;__constinit&lt;/code&gt; in earlier C++ modes, as an extension, to get a similar effect.&lt;/p&gt; &lt;h2&gt;Deprecated uses of &lt;code&gt;volatile&lt;/code&gt;&lt;/h2&gt; &lt;p&gt;Expressions that involve both loads and stores of a &lt;code&gt;volatile&lt;/code&gt; lvalue, such as &lt;code&gt;++&lt;/code&gt; or &lt;code&gt;+=&lt;/code&gt;, are &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1152r4.html"&gt;deprecated&lt;/a&gt;. The &lt;code&gt;volatile&lt;/code&gt;-qualified parameter and return types are also deprecated, so GCC 10 will warn in:&lt;/p&gt; &lt;pre&gt;void fn () { volatile int v = 42; // Load + store or just a load? ++v; // warning: deprecated } &lt;/pre&gt; &lt;h3&gt;Conversions to arrays of unknown bound&lt;/h3&gt; &lt;p&gt;Converting to an array of unknown bound is now &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0388r4.html"&gt;permitted&lt;/a&gt;, so the following code compiles:&lt;/p&gt; &lt;pre&gt;void f(int(&amp;#38;)[]); int arr[1]; void g() { f(arr); } int(&amp;#38;r)[] = arr; &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: Conversion in the other direction—&lt;em&gt;from&lt;/em&gt; arrays of unknown bound—currently is not allowed by the C++ standard.&lt;/p&gt; &lt;h3&gt;&lt;code&gt;constexpr new&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;This &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0784r7.html"&gt;feature&lt;/a&gt; allows dynamic memory allocation at compile time in a &lt;code&gt;constexpr&lt;/code&gt; context:&lt;/p&gt; &lt;pre&gt;constexpr auto fn () { int *p = new int{10}; // ... use p ... delete p; return 0; } int main () { constexpr auto i = fn (); } &lt;/pre&gt; &lt;p&gt;Note that the storage allocated at compile time in a &lt;code&gt;constexpr&lt;/code&gt; context must also be freed at compile time. And, given that &lt;code&gt;constexpr&lt;/code&gt; doesn&amp;#8217;t allow undefined behavior, &lt;code&gt;use-after-free&lt;/code&gt; is a compile-time error. The &lt;code&gt;new&lt;/code&gt; expression also can&amp;#8217;t throw. This feature paves the way for &lt;code&gt;constexpr&lt;/code&gt; standard containers such as &lt;code&gt;&amp;#60;vector&amp;#62;&lt;/code&gt; and &lt;code&gt;&amp;#60;string&amp;#62;&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;The &lt;code&gt;[[nodiscard]]&lt;/code&gt; attribute&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;[[nodiscard]]&lt;/code&gt; attribute now supports an optional argument, like so:&lt;/p&gt; &lt;pre&gt;[[nodiscard("unsafe")]] int *fn (); &lt;/pre&gt; &lt;p&gt;See &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1301r4.html"&gt;the proposal&lt;/a&gt; for details.&lt;/p&gt; &lt;h2&gt;CTAD extensions&lt;/h2&gt; &lt;p&gt;C++20 class template argument deduction (CTAD) now works for &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1814r0.html"&gt;alias templates&lt;/a&gt; and &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1816r0.pdf"&gt;aggregates&lt;/a&gt;, too:&lt;/p&gt; &lt;pre&gt;template &amp;#60;typename T&amp;#62; struct Aggr { T x; }; Aggr a = { 1 }; // works in GCC 10 template &amp;#60;typename T&amp;#62; struct NonAggr { NonAggr(); T x; }; NonAggr n = { 1 }; // error: deduction fails &lt;/pre&gt; &lt;h2&gt;Parenthesized initialization of aggregates&lt;/h2&gt; &lt;p&gt;You can now initialize an aggregate using a &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0960r3.html"&gt;parenthesized list of values&lt;/a&gt; such as &lt;code&gt;(1, 2, 3)&lt;/code&gt;. The behavior is similar to &lt;code&gt;{1, 2, 3}&lt;/code&gt;, but in parenthesized initialization, the following exceptions apply:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Narrowing conversions are permitted.&lt;/li&gt; &lt;li&gt;Designators (things like &lt;code&gt;.a = 10&lt;/code&gt;) are not permitted.&lt;/li&gt; &lt;li&gt;A temporary object bound to a reference does not have its lifetime extended.&lt;/li&gt; &lt;li&gt;There is no brace elision.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here&amp;#8217;s an example:&lt;/p&gt; &lt;pre&gt;struct A { int a, b; }; A a1{1, 2}; A a2(1, 2); // OK in GCC 10 -std=c++20 auto a3 = new A(1, 2); // OK in GCC 10 -std=c++20 &lt;/pre&gt; &lt;h3&gt;Trivial default initialization in &lt;code&gt;constexpr&lt;/code&gt; contexts&lt;/h3&gt; &lt;p&gt;This usage is now &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1331r2.pdf"&gt;allowed&lt;/a&gt; in C++20. As a result, a &lt;code&gt;constexpr&lt;/code&gt; constructor doesn&amp;#8217;t necessarily have to initialize all the fields (but reading an uninitialized object is, of course, still forbidden):&lt;/p&gt; &lt;pre&gt;struct S { int i; int u; constexpr S() : i{1} { } }; constexpr S s; // error: refers to an incompletely initialized variable S s2; // OK constexpr int fn (int n) { int a; a = 5; return a + n; } &lt;/pre&gt; &lt;h2&gt;&lt;code&gt;constexpr dynamic_cast&lt;/code&gt;&lt;/h2&gt; &lt;p&gt;In &lt;code&gt;constexpr&lt;/code&gt; contexts, you can now evaluate a &lt;code&gt;dynamic_cast&lt;/code&gt; &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1327r1.html"&gt;at compile time&lt;/a&gt;. Virtual function calls in constant expressions were already permitted, so this proposal made it valid to use a &lt;code&gt;constexpr&lt;/code&gt; &lt;code&gt;dynamic_cast&lt;/code&gt;, like this:&lt;/p&gt; &lt;pre&gt;struct B { virtual void baz () {} }; struct D : B { }; constexpr D d; constexpr B *b = const_cast&amp;#60;D*&amp;#62;(&amp;#38;d); static_assert(dynamic_cast&amp;#60;D*&amp;#62;(b) == &amp;#38;d); &lt;/pre&gt; &lt;p&gt;Previously, a &lt;code&gt;constexpr&lt;/code&gt; &lt;code&gt;dynamic_cast&lt;/code&gt; would have required a runtime call to a function defined by the C++ runtime library. Similarly, a polymorphic &lt;code&gt;typeid&lt;/code&gt; is now also allowed in &lt;code&gt;constexpr&lt;/code&gt; contexts.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: C++20 modules are not supported in GCC 10; they are still a work in progress (here is &lt;a target="_blank" rel="nofollow" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1103r3.pdf"&gt;a related proposal&lt;/a&gt;). We hope to include them in GCC 11.&lt;/p&gt; &lt;h2&gt;Additional updates&lt;/h2&gt; &lt;p&gt;In non-C++20 news, the C++ compiler now detects modifying constant objects in &lt;code&gt;constexpr&lt;/code&gt; evaluation, which is undefined behavior:&lt;/p&gt; &lt;pre&gt;constexpr int fn () { const int i = 5; const_cast&amp;#60;int &amp;#38;&amp;#62;(i) = 10; // error: modifying a const object return i; } constexpr int i = fn (); &lt;/pre&gt; &lt;p&gt;GCC also handles the case when a constant object under construction is being modified and doesn&amp;#8217;t emit an error in that case.&lt;/p&gt; &lt;h3&gt;Narrowing conversions&lt;/h3&gt; &lt;p&gt;Narrowing conversions are invalid in certain contexts, such as list initialization:&lt;/p&gt; &lt;pre&gt;int i{1.2}; &lt;/pre&gt; &lt;p&gt;GCC 10 is able to detect narrowing in more contexts where it&amp;#8217;s invalid, for instance, case values in a &lt;code&gt;switch&lt;/code&gt; statement:&lt;/p&gt; &lt;pre&gt;void g(int i) { switch (i) case __INT_MAX__ + 1u:; } &lt;/pre&gt; &lt;h3&gt;The &lt;code&gt;noexcept&lt;/code&gt; specifier&lt;/h3&gt; &lt;p&gt;GCC 10 properly treats the &lt;code&gt;noexcept&lt;/code&gt; specifier as a &lt;em&gt;complete-class context&lt;/em&gt;. As with member function bodies, default arguments, and non-static data member initializers, you can declare names used in a member function&amp;#8217;s &lt;code&gt;noexcept&lt;/code&gt; specifier later in the class body. The following valid C++ code could not be compiled with GCC 9, but it compiles with GCC 10:&lt;/p&gt; &lt;pre&gt;struct S { void foo() noexcept(b); static constexpr auto b = true; }; &lt;/pre&gt; &lt;h3&gt;The &lt;code&gt;deprecated&lt;/code&gt; attribute&lt;/h3&gt; &lt;p&gt;You can now use the &lt;code&gt;deprecated&lt;/code&gt; attribute on namespaces:&lt;/p&gt; &lt;pre&gt;namespace v0 [[deprecated("oh no")]] { int fn (); } void g () { int x = v0::fn (); // warning: v0 is deprecated } &lt;/pre&gt; &lt;p&gt;GCC 9 compiles this code but ignores the attribute, whereas GCC 10 correctly warns about using entities from the deprecated namespace.&lt;/p&gt; &lt;h3&gt;Defect report resolutions&lt;/h3&gt; &lt;p&gt;We resolved several defect reports (DRs) in GCC 10. One example is &lt;a target="_blank" rel="nofollow" href="http://wg21.link/cwg1710"&gt;DR 1710&lt;/a&gt;, which says that when we are naming a type, the &lt;code&gt;template&lt;/code&gt; keyword is optional. Therefore, the following test compiles without errors with GCC 10:&lt;/p&gt; &lt;pre&gt;template&amp;#60;typename T&amp;#62; struct S { void fn(typename T::template B&amp;#60;int&amp;#62;::template C&amp;#60;int&amp;#62;); void fn2(typename T::B&amp;#60;int&amp;#62;::template C&amp;#60;int&amp;#62;); void fn3(typename T::template B&amp;#60;int&amp;#62;::C&amp;#60;int&amp;#62;); void fn4(typename T::B&amp;#60;int&amp;#62;::C&amp;#60;int&amp;#62;); }; &lt;/pre&gt; &lt;p&gt;Another interesting DR is &lt;a target="_blank" rel="nofollow" href="http://wg21.link/cwg1307"&gt;DR 1307&lt;/a&gt;, which clarified how overload resolution ought to behave when it comes to choosing a better candidate based on the size-of-array initializer list. Consider the following test:&lt;/p&gt; &lt;pre&gt;void f(int const(&amp;#38;)[2]); void f(int const(&amp;#38;)[3]) = delete; void g() { f({1, 2}); } &lt;/pre&gt; &lt;p&gt;GCC 9 rejects this test because it can&amp;#8217;t decide which candidate is better. Yet it seems obvious that the first candidate is best (never mind the &lt;code&gt;= delete&lt;/code&gt; part; deleted functions participate in overload resolution). GCC 10 chooses this option, which lets the code compile.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;b&gt;Note&lt;/b&gt;: You can find the overall defect resolution status on the &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/projects/cxx-dr-status.html"&gt;C++ Defect Report Support in GCC&lt;/a&gt; page.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In GCC 11, we plan to finish up the remaining C++20 features. For progress so far, see the &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/projects/cxx-status.html#cxx2a"&gt;C++2a Language Features&lt;/a&gt; table on the &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/projects/cxx-status.html"&gt;C++ Standards Support in GCC&lt;/a&gt; page. GCC 11 will also switch the default dialect to C++17 (it has already happened). Please do not hesitate to &lt;a target="_blank" rel="nofollow" href="https://gcc.gnu.org/bugs/"&gt;file bugs&lt;/a&gt; in the meantime, and help us make GCC even better!&lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;I&amp;#8217;d like to thank my coworkers at Red Hat who made the GNU C++ compiler so much better, notably Jason Merrill, Jakub Jelinek, Patrick Palka, and Jonathan Wakely.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#38;linkname=New%20C%2B%2B%20features%20in%20GCC%2010" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fnew-c-features-in-gcc-10%2F&amp;#038;title=New%20C%2B%2B%20features%20in%20GCC%2010" data-a2a-url="https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/" data-a2a-title="New C++ features in GCC 10"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/"&gt;New C++ features in GCC 10&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JYKdxbvy15s" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The GNU Compiler Collection (GCC) 10.1 was released in May 2020. Like every other GCC release, this version brought many additions, improvements, bug fixes, and new features. Fedora 32 already ships GCC 10 as the system compiler, but it&amp;#8217;s also possible to try GCC 10 on other platforms (see godbolt.org, for example). Red Hat Enterprise [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/"&gt;New C++ features in GCC 10&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">1</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">749897</post-id><dc:creator>Marek Polacek</dc:creator><dc:date>2020-09-24T07:00:32Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/24/new-c-features-in-gcc-10/</feedburner:origLink></entry><entry><title>Set up continuous integration for .NET Core with OpenShift Pipelines</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QReTQpTE1tg/" /><category term=".NET Core" /><category term="C#" /><category term="CI/CD" /><category term="Kubernetes" /><category term=".net core ci" /><category term="ci/cd pipeline" /><category term="CodeReady Containers" /><category term="openshift" /><category term="tekton pipelines" /><author><name>Omair Majid</name></author><id>https://developers.redhat.com/blog/?p=747217</id><updated>2020-09-24T07:00:09Z</updated><published>2020-09-24T07:00:09Z</published><content type="html">&lt;p&gt;Have you ever wanted to set up &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration&lt;/a&gt; (CI) for &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET Core&lt;/a&gt; in a cloud-native way, but you didn&amp;#8217;t know where to start? This article provides an overview, examples, and suggestions for developers who want to get started setting up a functioning &lt;a href="https://developers.redhat.com/blog/2020/08/14/introduction-to-cloud-native-ci-cd-with-tekton-kubecon-europe-2020/"&gt;cloud-native CI&lt;/a&gt; system for .NET Core.&lt;/p&gt; &lt;p&gt;We will use the new &lt;a href="https://developers.redhat.com/blog/2020/04/30/creating-pipelines-with-openshift-4-4s-new-pipeline-builder-and-tekton-pipelines/"&gt;Red Hat OpenShift Pipelines&lt;/a&gt; feature to implement .NET Core CI. &lt;a href="https://developers.redhat.com/blog/2020/04/27/modern-web-applications-on-openshift-part-4-openshift-pipelines"&gt;OpenShift Pipelines&lt;/a&gt; are based on the open source &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/pipeline#-tekton-pipelines"&gt;Tekton&lt;/a&gt; project. OpenShift Pipelines provide a cloud-native way to define a pipeline to build, test, deploy, and roll out your applications in a continuous integration workflow.&lt;/p&gt; &lt;p&gt;In this article, you will learn how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Set up a simple .NET Core application.&lt;/li&gt; &lt;li&gt;Install OpenShift Pipelines on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Create a simple pipeline manually.&lt;/li&gt; &lt;li&gt;Create a Source-to-Image (S2I)-based pipeline.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;span id="more-747217"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You will need cluster-administrator access to an OpenShift instance to be able to access the example application and follow all of the steps described in this article. If you don&amp;#8217;t have access to an OpenShift instance, or if you don&amp;#8217;t have cluster-admin privileges, you can run an OpenShift instance locally on your machine using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt;. Running OpenShift locally should be as easy as &lt;code&gt;crc setup&lt;/code&gt; followed by &lt;code&gt;crc start&lt;/code&gt;. Also, be sure to &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/cli_reference/openshift_cli/getting-started-cli.html"&gt;install&lt;/a&gt; the &lt;code&gt;oc&lt;/code&gt; tool; we will use it throughout the examples.&lt;/p&gt; &lt;p&gt;When I wrote this article, I was using:&lt;/p&gt; &lt;pre&gt;$ crc version CodeReady Containers version: 1.12.0+6710aff OpenShift version: 4.4.8 (embedded in binary) &lt;/pre&gt; &lt;h2&gt;The example application&lt;/h2&gt; &lt;p&gt;To start, let&amp;#8217;s review the .NET Core project that we will use for the remainder of the article. It&amp;#8217;s a simple &amp;#8220;Hello, World&amp;#8221;-style web application that is built on top of ASP.NET Core.&lt;/p&gt; &lt;p&gt;The complete application is available from the Red Hat Developer &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex"&gt;s2i-dotnetcore-ex&lt;/a&gt; GitHub repository, in the branch &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/tree/dotnetcore-3.1-openshift-manual-pipeline"&gt;dotnetcore-3.1-openshift-manual-pipeline&lt;/a&gt;. You can use this repository and branch directly for the steps described in this article. If you want to make changes and test how they affect your pipeline, you can fork the repository and use the fork.&lt;/p&gt; &lt;p&gt;Everything is included in the source repository, including everything we need for continuous integration. There are at least a couple of advantages to this approach: Our complete setup is tracked and reproducible, and we can easily code-review changes we might do later on, including any changes to our CI pipeline.&lt;/p&gt; &lt;h3&gt;Project directories&lt;/h3&gt; &lt;p&gt;Before we continue, let&amp;#8217;s review the &lt;code&gt;dotnetcore-3.1-openshift-manual-pipeline&lt;/code&gt; branch of the repository.&lt;/p&gt; &lt;p&gt;This project contains two main directories: &lt;code&gt;app&lt;/code&gt; and &lt;code&gt;app.tests&lt;/code&gt;. The &lt;code&gt;app&lt;/code&gt; directory contains the application code. This app was essentially created by &lt;code&gt;dotnet new mvc&lt;/code&gt;. The &lt;code&gt;app.tests&lt;/code&gt; directory contains unit tests that we want to run when we build the app. This was created by &lt;code&gt;dotnet new xunit&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Another directory called &lt;code&gt;ci&lt;/code&gt; contains the configuration we will use for our CI setup. I will describe the files in that directory in detail as we go through the examples.&lt;/p&gt; &lt;p&gt;Now, let&amp;#8217;s get to the fun part.&lt;/p&gt; &lt;h2&gt;Install OpenShift Pipelines&lt;/h2&gt; &lt;p&gt;Log into the OpenShift cluster as an administrator and install OpenShift Pipelines. You can use the &lt;b&gt;OperatorHub&lt;/b&gt; in your OpenShift console, as shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_747247" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01.png"&gt;&lt;img aria-describedby="caption-attachment-747247" class="wp-image-747247" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01.png" alt="The OpenShift Pipelines Operator installation screen." width="640" height="472" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01.png 913w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01-300x221.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-01-768x567.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747247" class="wp-caption-text"&gt;Figure 1: Use the OpenShift OperatorHub to install OpenShift Pipelines.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;See the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/pipelines/installing-pipelines.html"&gt;OpenShift documentation&lt;/a&gt; for a complete guide to installing the OpenShift Pipelines Operator.&lt;/p&gt; &lt;h2&gt;Develop a pipeline manually&lt;/h2&gt; &lt;p&gt;Now that we have our .NET Core application code and an OpenShift instance ready to use, we can create an Openshift Pipelines-based system for building this code. For our first example, we&amp;#8217;ll create a pipeline manually.&lt;/p&gt; &lt;p&gt;Before we create the pipeline, let&amp;#8217;s start with a few things you need to know about OpenShift Pipelines.&lt;/p&gt; &lt;h3&gt;About OpenShift Pipelines&lt;/h3&gt; &lt;p&gt;OpenShift Pipelines offer a flexible way to build and deploy applications. OpenShift Pipelines are not opinionated: You can do pretty much anything you want as part of a pipeline. To be this flexible, OpenShift Pipelines lets us define several different types of objects. These objects represent various parts of the pipeline, such as inputs, output, stages, and even connections between these elements. We can assemble these objects however we need to, to get the build-test-deploy pipeline that we want.&lt;/p&gt; &lt;p&gt;Here are a few types of objects that we will use:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An &lt;strong&gt;OpenShift Pipelines &lt;code&gt;Task&lt;/code&gt;&lt;/strong&gt; is a collection of &lt;code&gt;step&lt;/code&gt;s that we want to run in order. There are many predefined &lt;code&gt;Task&lt;/code&gt;s, and we can make more as we need them. &lt;code&gt;Task&lt;/code&gt;s can have inputs and outputs. They can also have parameters that control aspects of the &lt;code&gt;Task&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/strong&gt; is a collection of &lt;code&gt;Task&lt;/code&gt;s. It lets us group &lt;code&gt;Task&lt;/code&gt;s and run them in a particular order, and it lets us connect the output of one &lt;code&gt;Task&lt;/code&gt; to the input of another.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;PipelineResource&lt;/code&gt;&lt;/strong&gt; indicates a &lt;code&gt;Task&lt;/code&gt; or&amp;#8217;s input or output. There are several types, including &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;image&lt;/code&gt;. The &lt;code&gt;git&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; indicates a Git repository, which is often used as in the input to a &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;Task&lt;/code&gt;. An &lt;code&gt;image&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; indicates a container image that we can use via the built-in OpenShift container registry.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These objects describe what a &lt;code&gt;Pipeline&lt;/code&gt; is. We need at least two more things to run a pipeline. Creating and running a pipeline are two distinct operations in OpenShift. You can create a pipeline and never run it. Or, you can re-run the same pipeline multiple times.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;TaskRun&lt;/code&gt; &lt;/strong&gt;describes how to run a &lt;code&gt;Task&lt;/code&gt;. It connects the inputs and outputs of a &lt;code&gt;Task&lt;/code&gt; with &lt;code&gt;PipelineResource&lt;/code&gt;s.&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;&lt;code&gt;PipelineRun&lt;/code&gt;&lt;/strong&gt; describes how to run a &lt;code&gt;Pipeline&lt;/code&gt;. It connects the pipeline&amp;#8217;s inputs and outputs with &lt;code&gt;PipelineResource&lt;/code&gt;s. A &lt;code&gt;PipelineRun&lt;/code&gt; implicitly contains one or more &lt;code&gt;TaskRun&lt;/code&gt;s, so we can skip creating &lt;code&gt;TaskRun&lt;/code&gt;s manually.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now we&amp;#8217;re ready to put all of these elements into action.&lt;/p&gt; &lt;h3&gt;Step 1: Create a new pipeline project&lt;/h3&gt; &lt;p&gt;First, we&amp;#8217;ll create a new project to work in OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc new-project dotnet-pipeline-app&lt;/pre&gt; &lt;p&gt;To make a functional pipeline, we will need to define a few objects that we can add to a single file, &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/simple/simple-pipeline.yaml"&gt;simple-pipeline.yaml&lt;/a&gt;. This file is located in the &lt;code&gt;ci/simple/simple-pipeline.yaml&lt;/code&gt; file in the repository that we are using. To separate these objects, we will use a dash (&lt;code&gt;---&lt;/code&gt;) by itself on a single line.&lt;/p&gt; &lt;h3&gt;Step 2: Define a PipelineResource&lt;/h3&gt; &lt;p&gt;Next, we&amp;#8217;ll define a &lt;code&gt;PipelineResource&lt;/code&gt;. The &lt;code&gt;PipelineResource&lt;/code&gt; will provide the source Git repository to the rest of the pipeline:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: simple-dotnet-project-source spec: type: git params: - name: revision value: dotnetcore-3.1-openshift-manual-pipeline - name: url value: https://github.com/redhat-developer/s2i-dotnetcore-ex &lt;/pre&gt; &lt;p&gt;An object of &lt;code&gt;kind: PipelineResource&lt;/code&gt; has a name (&lt;code&gt;metadata.name&lt;/code&gt;) and a type of resource (&lt;code&gt;spec.type&lt;/code&gt;). Here, we use the type &lt;code&gt;git&lt;/code&gt; to indicate that this &lt;code&gt;PipelineResource&lt;/code&gt; represents a Git repository. The parameters to this &lt;code&gt;PipelineResource&lt;/code&gt; specify the Git repository that we want to use.&lt;/p&gt; &lt;p&gt;If you are using your own fork, please adjust the URL and branch name.&lt;/p&gt; &lt;h3&gt;Step 3: Define the pipeline&lt;/h3&gt; &lt;p&gt;Next, we define a pipeline. The &lt;code&gt;Pipeline&lt;/code&gt; instance will coordinate all of the tasks that we want to run:&lt;/p&gt; &lt;pre&gt;apapiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: simple-dotnet-pipeline spec: resources: - name: source-repository type: git tasks: - name: simple-build-and-test taskRef: name: simple-publish resources: inputs: - name: source resource: source-repository &lt;/pre&gt; &lt;p&gt;This &lt;code&gt;Pipeline&lt;/code&gt; uses a resource (a &lt;code&gt;PipelineResource&lt;/code&gt;) with the name of &lt;code&gt;source-repository&lt;/code&gt;, which should be a &lt;code&gt;git&lt;/code&gt; repository. It uses just one &lt;code&gt;Task&lt;/code&gt; (via the &lt;code&gt;taskRef&lt;/code&gt; with a &lt;code&gt;name&lt;/code&gt; of &lt;code&gt;simple-publish&lt;/code&gt;) that will build our source code. We will connect the input of the pipeline (&lt;code&gt;source-repository&lt;/code&gt; resource) to the input of the task (&lt;code&gt;source&lt;/code&gt; input resource).&lt;/p&gt; &lt;h3&gt;Step 4: Define and build a task&lt;/h3&gt; &lt;p&gt;Next, we define a simple &lt;code&gt;Task&lt;/code&gt; that takes a &lt;code&gt;git&lt;/code&gt; source repository. Then, we build the task:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: simple-publish spec: resources: inputs: - name: source type: git steps: - name: simple-dotnet-publish image: registry.access.redhat.com/ubi8/dotnet-31 # .NET Core SDK securityContext: runAsUser: 0  # UBI 8 images generally run as non-root script: | #!/usr/bin/bash dotnet --info cd source dotnet publish -c Release -r linux-x64 --self-contained false "app/app.csproj" &lt;/pre&gt; &lt;p&gt;This &lt;code&gt;Task&lt;/code&gt; takes a &lt;code&gt;PipelineResource&lt;/code&gt; of type &lt;code&gt;git&lt;/code&gt; that represents the application source code. It runs the listed &lt;code&gt;steps&lt;/code&gt; to build the source code using the &lt;code&gt;ubi8/dotnet-31&lt;/code&gt; container image. This container image is published by Red Hat and is optimized for building .NET Core applications in &lt;a href="https://developers.redhat.com/topics/linux"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) and OpenShift. We also specify, via &lt;code&gt;script&lt;/code&gt;, the exact steps to build our .NET Core application, which is essentially just a &lt;code&gt;dotnet publish&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Step 5: Describe and apply the pipeline&lt;/h3&gt; &lt;p&gt;We have everything that we need to describe our pipeline. Our final &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/simple/simple-pipeline.yaml"&gt;ci/simple/simple-pipeline.yaml&lt;/a&gt; file should look like this:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: simple-dotnet-project-source spec: type: git params: - name: revision value: dotnetcore-3.1-openshift-manual-pipeline - name: url value: https://github.com/redhat-developer/s2i-dotnetcore-ex --- apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: simple-dotnet-pipeline spec: resources: - name: source-repository type: git tasks: - name: simple-build-and-test taskRef: name: simple-publish resources: inputs: - name: source resource: source-repository --- apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: simple-publish spec: resources: inputs: - name: source type: git steps: - name: simple-dotnet-publish image: registry.access.redhat.com/ubi8/dotnet-31 # .NET Core SDK securityContext: runAsUser: 0  # UBI 8 images generally run as non-root script: | #!/usr/bin/bash dotnet --info cd source dotnet publish -c Release -r linux-x64 --self-contained false "app/app.csproj" &lt;/pre&gt; &lt;p&gt;We can now apply it to our OpenShift instance:&lt;/p&gt; &lt;pre&gt;$ oc apply -f ci/simple/simple-pipeline.yaml&lt;/pre&gt; &lt;p&gt;Applying the pipeline to our OpenShift instance makes OpenShift modify its currently-active configuration to match what we have specified in the YAML file. That includes creating any objects (such as &lt;code&gt;Pipeline&lt;/code&gt;s, &lt;code&gt;PipelineResource&lt;/code&gt;s, and &lt;code&gt;Task&lt;/code&gt;s) that did not previously exist. If objects of the same name and types already exist, they will be modified to match what we have specified in our YAML file.&lt;/p&gt; &lt;p&gt;If you look in &lt;b&gt;Pipelines&lt;/b&gt; section of the OpenShift Developer console (&lt;code&gt;crc console&lt;/code&gt; if you are using CodeReady Containers), you should see that this pipeline is now available, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_747347" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02.png"&gt;&lt;img aria-describedby="caption-attachment-747347" class="wp-image-747347 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-1024x401.png" alt="The OpenShift dashboard showing the newly created pipeline." width="640" height="251" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-1024x401.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-300x117.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02-768x301.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-02.png 1344w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747347" class="wp-caption-text"&gt;Figure 2: The new pipeline in the OpenShift developer console.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Did you notice that the pipeline is not running? We&amp;#8217;ll deal with that next.&lt;/p&gt; &lt;h3&gt;Step 6: Run the pipeline&lt;/h3&gt; &lt;p&gt;To run the pipeline, we will create a &lt;code&gt;PipelineRun&lt;/code&gt; object that associates the &lt;code&gt;PipelineResource&lt;/code&gt; with the &lt;code&gt;Pipeline&lt;/code&gt; itself, and actually runs everything. The &lt;code&gt;PipelineRun&lt;/code&gt; object for this example is &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/simple/run-simple-pipeline.yaml"&gt;ci/simple/run-simple-pipeline.yaml&lt;/a&gt;, as shown here:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: run-simple-dotnet-pipeline- spec: pipelineRef: name: simple-dotnet-pipeline resources: - name: source-repository resourceRef: name: simple-dotnet-project-source &lt;/pre&gt; &lt;p&gt;This YAML file defines an object of &lt;code&gt;kind: PipelineRun&lt;/code&gt;, which uses the &lt;code&gt;simple-dotnet-pipeline&lt;/code&gt; (indicated with the &lt;code&gt;pipelineRef&lt;/code&gt;). It associates the &lt;code&gt;simple-dotnet-project-source&lt;/code&gt; &lt;code&gt;PipelineResource&lt;/code&gt; that we defined earlier with the input resource that the &lt;code&gt;simple-dotnet-pipeline&lt;/code&gt; expects.&lt;/p&gt; &lt;p&gt;Next, we will use &lt;code&gt;oc create&lt;/code&gt; (instead of &lt;code&gt;oc apply&lt;/code&gt;) to create and run the pipeline:&lt;/p&gt; &lt;pre&gt;$ oc create -f ci/simple/run-simple-pipeline.yaml&lt;/pre&gt; &lt;p&gt;We need to use &lt;code&gt;oc create&lt;/code&gt; here because each &lt;code&gt;PipelineRun&lt;/code&gt; represents an actual invocation of the pipeline. We don&amp;#8217;t want to &lt;code&gt;apply&lt;/code&gt; an update to a previous run of a &lt;code&gt;Pipeline&lt;/code&gt;. We want to run the pipeline (again).&lt;/p&gt; &lt;p&gt;This is also why we used &lt;code&gt;generateName&lt;/code&gt; instead of &lt;code&gt;name&lt;/code&gt; when defining the pipeline YAML. Each OpenShift object has a unique name, including a &lt;code&gt;PipelineRun&lt;/code&gt;. If we gave our pipeline a concrete name, we wouldn&amp;#8217;t be able to &lt;code&gt;oc create&lt;/code&gt; it again. To work around this, we tell OpenShift to generate a new name, prefixing it with our provided value. This allows us to run &lt;code&gt;oc create -f ci/simple/simple-pipeline.yaml&lt;/code&gt; multiple times. Each time we enter this command, it will run the pipeline in OpenShift.&lt;/p&gt; &lt;p&gt;You should now see the pipeline running in the OpenShift console, as shown in Figure 3. Wait a little bit, and it will complete.&lt;/p&gt; &lt;div id="attachment_747357" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03.png"&gt;&lt;img aria-describedby="caption-attachment-747357" class="wp-image-747357 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-1024x520.png" alt="A terminal showing the pipeline running." width="640" height="325" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-1024x520.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03-768x390.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-03.png 1081w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747357" class="wp-caption-text"&gt;Figure 3: The manually created pipeline running in the OpenShift console.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Develop an S2I-based pipeline&lt;/h2&gt; &lt;p&gt;In the previous section, we created a pipeline manually. For that, we had to write out all the steps to publish our application. We had to list out the exact steps to build our code, as well as the container image we wanted to use to build it. We also would have had to list out any other steps, such as running tests.&lt;/p&gt; &lt;p&gt;We can make our lives easier by re-using predefined tasks. The predefined &lt;code&gt;s2i-dotnet-3&lt;/code&gt; &lt;code&gt;Task&lt;/code&gt; already knows how to build a .NET Core application using the &lt;code&gt;s2i-dotnetcore&lt;/code&gt; images. In this section, you&amp;#8217;ll learn how to build an S2I-based pipeline.&lt;/p&gt; &lt;h3&gt;Step 1: Install the .NET Core &lt;code&gt;s2i&lt;/code&gt; tasks&lt;/h3&gt; &lt;p&gt;Let&amp;#8217;s start by installing the &lt;code&gt;s2i&lt;/code&gt; tasks for .NET Core 3.x:&lt;/p&gt; &lt;pre&gt;$ oc apply -f https://raw.githubusercontent.com/openshift/pipelines-catalog/master/task/s2i-dotnet-3-pr/0.1/s2i-dotnet-3-pr.yaml &lt;/pre&gt; &lt;h3&gt;Step 2: Add an &lt;code&gt;.s2i/environment&lt;/code&gt; file to the repository&lt;/h3&gt; &lt;p&gt;We will also need to use a few new files in our code repository to tell the S2I build system what .NET Core projects to use for testing and deployment. We can do that by adding a &lt;code&gt;.s2i/environment&lt;/code&gt; file to the repository with the following two lines:&lt;/p&gt; &lt;pre&gt;DOTNET_STARTUP_PROJECT=app/app.csproj DOTNET_TEST_PROJECTS=app.tests/app.tests.csproj &lt;/pre&gt; &lt;p&gt;&lt;code&gt;DOTNET_STARTUP_PROJECT&lt;/code&gt; specifies what project (or application) we want to run in the final built container. &lt;code&gt;DOTNET_TEST_PROJECTS&lt;/code&gt; specifies which projects (if any) we want to run to test our code before we build the application container.&lt;/p&gt; &lt;h3&gt;Step 3: Define the pipeline&lt;/h3&gt; &lt;p&gt;Now, let&amp;#8217;s define a &lt;code&gt;Pipeline&lt;/code&gt;, &lt;code&gt;PipelineResource&lt;/code&gt;s, and &lt;code&gt;Task&lt;/code&gt;s in a single pipeline file. This time, we will use the &lt;code&gt;s2i-dotnet-3-pr&lt;/code&gt; task. We will also include another &lt;code&gt;PipelineResource&lt;/code&gt; with &lt;code&gt;type: image&lt;/code&gt; that represents the output container image. This pipeline will build the source code and then push the just-built application image into OpenShift&amp;#8217;s built-in container registry. Our &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/s2i/s2i-pipeline.yaml"&gt;ci/s2i/s2i-pipeline.yaml&lt;/a&gt; should look like this:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: s2i-dotnet-project-source spec: type: git params: - name: revision value: dotnetcore-3.1-openshift-manual-pipeline - name: url value: https://github.com/redhat-developer/s2i-dotnetcore-ex --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: s2i-dotnet-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/dotnet-pipeline-app/app:latest --- apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: s2i-dotnet-pipeline spec: resources: - name: source-repository type: git - name: image type: image tasks: - name: s2i-build-source taskRef: name: s2i-dotnet-3-pr params: - name: TLSVERIFY value: "false" resources: inputs: - name: source resource: source-repository outputs: - name: image resource: image &lt;/pre&gt; &lt;p&gt;Note how we provided the &lt;code&gt;s2i-dotnet-3-pr&lt;/code&gt; task with the &lt;code&gt;source-repository&lt;/code&gt; resource and the &lt;code&gt;image&lt;/code&gt; resource. This task already knows how to build a .NET Core application from source code (based on the contents of the &lt;code&gt;.s2i/environment&lt;/code&gt; file) and create a container image that includes the just-built application.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please keep in mind that you can only push the container image to the same namespace as your OpenShift project, which in this case is &lt;code&gt;dotnet-pipeline-app&lt;/code&gt;. If we tried using a container name that placed the container image somewhere other than &lt;code&gt;/dotnet-pipeline-app/&lt;/code&gt;, we would encounter permission issues.&lt;/p&gt; &lt;h3&gt;Step 4: Apply the pipeline in OpenShift&lt;/h3&gt; &lt;p&gt;Now let&amp;#8217;s set up this pipeline in OpenShift:&lt;/p&gt; &lt;pre&gt;$ oc apply -f ci/s2i/s2i-pipeline.yaml &lt;/pre&gt; &lt;p&gt;The new &lt;code&gt;s2i-dotnet-pipeline&lt;/code&gt; should now show up in the &lt;b&gt;Pipelines&lt;/b&gt; section of the OpenShift developer console, as shown in Figure 4.&lt;/p&gt; &lt;div id="attachment_747387" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04.png"&gt;&lt;img aria-describedby="caption-attachment-747387" class="wp-image-747387 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-1024x445.png" alt="The OpenShift dashboard showing the S2I pipeline running." width="640" height="278" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-1024x445.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-300x130.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04-768x333.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-04.png 1085w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747387" class="wp-caption-text"&gt;Figure 4: The new pipeline in the OpenShift developer console.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Step 5: Run the pipeline&lt;/h3&gt; &lt;p&gt;To run this pipeline, we will create another &lt;code&gt;PipelineRun&lt;/code&gt;, just like we did in the previous example. Here&amp;#8217;s our ci/s2i/run-s2i-pipeline. &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore-ex/blob/dotnetcore-3.1-openshift-manual-pipeline/ci/s2i/run-s2i-pipeline.yaml"&gt;yaml&lt;/a&gt; file:&lt;/p&gt; &lt;pre&gt;apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: run-s2i-dotnet-pipeline- spec: serviceAccountName: pipeline pipelineRef: name: s2i-dotnet-pipeline resources: - name: source-repository resourceRef: name: s2i-dotnet-project-source - name: image resourceRef: name: s2i-dotnet-image &lt;/pre&gt; &lt;p&gt;Run it with:&lt;/p&gt; &lt;pre&gt;$ oc create -f ci/s2i/run-s2i-pipeline.yaml &lt;/pre&gt; &lt;p&gt;When you look at the OpenShift developer console, you can see the pipeline running (or having completed). The pipeline log should look something like what&amp;#8217;s shown in Figure 5:&lt;/p&gt; &lt;div id="attachment_747407" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05.png"&gt;&lt;img aria-describedby="caption-attachment-747407" class="wp-image-747407 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-1024x771.png" alt="The OpenShift terminal showing the S2I pipeline running." width="640" height="482" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-1024x771.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-300x226.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05-768x578.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/07/blog-image-05.png 1088w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-747407" class="wp-caption-text"&gt;Figure 5: The new pipeline running in the OpenShift developer console.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;This time, we can see that the pipeline started up, ran all of our tests, published our application, created a container image with it, and pushed the just-built container into the built-in container registry in OpenShift.&lt;/p&gt; &lt;p&gt;Nice, isn&amp;#8217;t it?&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you followed along with this article, then you should now know how to create a code-first implementation of cloud-ready CI pipelines to build, test, and publish container images for your .NET Core applications. Now that you are familiar with this procedure, why not try it out for your applications?&lt;/p&gt; &lt;p&gt;If you are looking for ideas to further what you&amp;#8217;ve learned, here are some suggestions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We triggered all of our pipeline runs manually, but that&amp;#8217;s obviously not what you want for a real continuous integration setup. Consider extending the pipeline to run &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/pipelines/creating-applications-with-cicd-pipelines.html#about-triggers_creating-applications-with-cicd-pipelines"&gt;on triggers and webhooks&lt;/a&gt; from your source repository.&lt;/li&gt; &lt;li&gt;We limited our CI pipeline to stop after building and publishing container images. Try creating a pipeline that goes all the way to deploying your project in OpenShift.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Additional resources&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Learn more about Red Hat &lt;a target="_blank" rel="nofollow" href="https://code-ready.github.io/crc"&gt;CodeReady Containers&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get complete instructions for &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.4/pipelines/creating-applications-with-cicd-pipelines.html"&gt;creating applications with OpenShift Pipelines&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Check out the &lt;a target="_blank" rel="nofollow" href="https://github.com/tektoncd/catalog"&gt;Tekton Task/Pipelines catalog&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Also, see the &lt;a target="_blank" rel="nofollow" href="https://github.com/openshift/pipelines-catalog"&gt;OpenShift Pipelines catalog&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get the &lt;a target="_blank" rel="nofollow" href="https://github.com/openshift/pipelines-catalog/tree/master/task"&gt;s2i dotnet task&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Get the &lt;a target="_blank" rel="nofollow" href="https://github.com/redhat-developer/s2i-dotnetcore"&gt;S2I container images&lt;/a&gt; for .NET Core.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#38;linkname=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F24%2Fset-up-continuous-integration-for-net-core-with-openshift-pipelines%2F&amp;#038;title=Set%20up%20continuous%20integration%20for%20.NET%20Core%20with%20OpenShift%20Pipelines" data-a2a-url="https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/" data-a2a-title="Set up continuous integration for .NET Core with OpenShift Pipelines"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/"&gt;Set up continuous integration for .NET Core with OpenShift Pipelines&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QReTQpTE1tg" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Have you ever wanted to set up continuous integration (CI) for .NET Core in a cloud-native way, but you didn&amp;#8217;t know where to start? This article provides an overview, examples, and suggestions for developers who want to get started setting up a functioning cloud-native CI system for .NET Core. We will use the new Red [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/"&gt;Set up continuous integration for .NET Core with OpenShift Pipelines&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">1</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">747217</post-id><dc:creator>Omair Majid</dc:creator><dc:date>2020-09-24T07:00:09Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/24/set-up-continuous-integration-for-net-core-with-openshift-pipelines/</feedburner:origLink></entry><entry><title>Payments Architecture - Immediate Payments Example</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/IjfMUYbRB_w/payments-architecture-immediate-payments-example.html" /><category term="Architecture Blueprints" scheme="searchisko:content:tags" /><category term="Automate" scheme="searchisko:content:tags" /><category term="Decision Manager" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_ericschabell" scheme="searchisko:content:tags" /><category term="FUSE" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="JBossAMQ" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Process Automation Manager" scheme="searchisko:content:tags" /><author><name>Eric D. Schabell</name></author><id>searchisko:content:id:jbossorg_blog-payments_architecture_immediate_payments_example</id><updated>2020-09-24T09:10:26Z</updated><published>2020-09-24T05:00:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; margin-right: 1em;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-NhImz2b--gU/X1jsrXrG_jI/AAAAAAAAxeI/2I4wj4AD4YUcuxWk1-464UVs5OiejZFwQCNcBGAsYHQ/s1600/christiann-koepke-0jPuWm8_9wY-unsplash.jpg" style="clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;"&gt;&lt;img alt="payments architecture" border="0" data-original-height="1067" data-original-width="1600" height="213" src="https://1.bp.blogspot.com/-NhImz2b--gU/X1jsrXrG_jI/AAAAAAAAxeI/2I4wj4AD4YUcuxWk1-464UVs5OiejZFwQCNcBGAsYHQ/s320/christiann-koepke-0jPuWm8_9wY-unsplash.jpg" title="" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="font-size: 12.8px; text-align: center;"&gt;Part 3 - Immediate payments&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Cloud technology is changing the way payment services are architectured. In this series we will be presenting insight from our customers on adopting open source and cloud technology to modernize their payment service.&lt;br /&gt;&lt;br /&gt;So far we've presented research-based architectural blueprints of&amp;nbsp;&lt;a href="http://www.schabell.org/2018/11/integration-key-to-customer-experience-introduction.html" target="_blank"&gt;omnichannel customer experience&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.schabell.org/2020/01/integrating-saas-applications-an-introduction.html" target="_blank"&gt;integrating with SaaS applications&lt;/a&gt;, and&amp;nbsp;&lt;a href="https://www.schabell.org/2020/05/cloud-native-development-a-blueprint.html" target="_blank"&gt;cloud-native development solutions&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;In &lt;a href="https://www.schabell.org/2020/09/financial-payments-architecture-common-elements.html" target="_blank"&gt;the previous article&lt;/a&gt; in this series we explored the common architectural elements found in a payments logical architecture.&lt;br /&gt;&lt;br /&gt;In this article we'll walk through an immediate payments physical architecture,&amp;nbsp; laying out what a successful payments solution looks like in practice.&lt;br /&gt;&lt;a name='more'&gt;&lt;/a&gt;&lt;br /&gt;&lt;h3 style="text-align: left;"&gt;Blueprints&lt;/h3&gt;&lt;div&gt;As a reminder, the architectural details covered here are base on real customer integration solutions using open source technologies.&lt;br /&gt;&lt;br /&gt;The example scenario presented here is a generic common blueprint that was uncovered researching customer solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details.&lt;br /&gt;&lt;br /&gt;This section covers the visual representations as presented. There are many ways to represent each element in this architectural blueprint, but we've chosen icons, text and colors that I hope are going to make it all easy to absorb. Feel free to post comments at the bottom of this post, or &lt;a href="https://www.schabell.org/p/contact.html" target="_blank"&gt;contact us&lt;/a&gt; directly with your feedback.&lt;br /&gt;&lt;br /&gt;Now let's take a look at the details in this blueprint and outline the example.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div&gt;&lt;h3 style="text-align: left;"&gt;Immediate payments&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;The example blueprint shown on the right entitled &lt;i&gt;Immediate payments network example&lt;/i&gt;&amp;nbsp;outlines how an immediate payments solution is applied to a physical architecture. Note that this diagram is focusing on the highest level of the immediate payments solution and the element groupings that apply to this process.&lt;br /&gt;&lt;a href="https://1.bp.blogspot.com/-HYFXb2JUxzE/X2CtZHQlN5I/AAAAAAAAxgk/5gBIv7jWsNcPccDsOgeVkT0t9ec98zY1ACNcBGAsYHQ/s1600/payments-immediate-payments-network-sd.png" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img alt="payments architecture" border="0" data-original-height="900" data-original-width="1600" height="180" src="https://1.bp.blogspot.com/-HYFXb2JUxzE/X2CtZHQlN5I/AAAAAAAAxgk/5gBIv7jWsNcPccDsOgeVkT0t9ec98zY1ACNcBGAsYHQ/s320/payments-immediate-payments-network-sd.png" title="" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;In this example, starting from the top left corner, a user sends an event or message to execute a payment as an entry point. The users can be mobile, web, or any external device / application that acts as the entry point with the organizations payments architecture.&lt;br /&gt;&lt;br /&gt;This request to execute payments connects to your services through the &lt;i&gt;payments API&lt;/i&gt;. This is the bridge to the internal central p&lt;i&gt;ayments event streams&lt;/i&gt;, where streams are managed to determine what selection or sub-selection of actions need to be taken. For practical purposes, we'll proceed through this architecture as if all selections are necessary to ensure coverage of all elements and services.&lt;br /&gt;&lt;br /&gt;The first action taken is validation of the incoming payments request, using the &lt;i&gt;validation microservices &lt;/i&gt;providing integration to all needed systems in an organization to validate funds, customers (users), and more. Once validation is completed a message is sent back to the &lt;i&gt;payments event streams&lt;/i&gt;&amp;nbsp;for further processing.&lt;br /&gt;&lt;br /&gt;The next two phases of the payments architecture are not always necessary as they depend on validation results. If there are any issues or flags raised on a payment request then the next step would be to trigger the use of the &lt;i&gt;anti-money laundering (AML) microservices &lt;/i&gt;or &lt;i&gt;fraud detection microservices. &lt;/i&gt;Each of these collections determine if action needs to be taken, if payment requests need to be blocked, if the user needs to be reported, and possibly blocking funds. Both of these service areas are leveraging &lt;i&gt;data caching&lt;/i&gt;&amp;nbsp;to ensure current data is available to the decision management tooling used to back these processing phases. Any results are then pushed back to the &lt;i&gt;payments event streams &lt;/i&gt;for further processing.&lt;br /&gt;&lt;br /&gt;Following the process elements to the right we'll arrive at the &lt;i&gt;clearing microservices&lt;/i&gt;&amp;nbsp;where processing for actual payment funds planning where accounts are debited before routing the funds to the requested parties. These results are sent back to the &lt;i&gt;payments event streams &lt;/i&gt;for further processing.&lt;br /&gt;&lt;br /&gt;Finally, &lt;i&gt;routing microservices &lt;/i&gt;are accessed to ensure the funds from the processed payments are distributed to the indicated parties through the available &lt;i&gt;payments network.&lt;/i&gt;&amp;nbsp;Note that the payments network is shown as an external secure cloud element, intended to indicate only that it's an external network and dependent on the region the solution is being deployed in for specifics as to the connection and data protocols used.&lt;br /&gt;&lt;a href="https://1.bp.blogspot.com/-qrlhtf3vujE/X2Ctmh0pIyI/AAAAAAAAxgo/jomUYBg9cak0uOrB7rxz1JjowWWVGwT4ACNcBGAsYHQ/s1600/payments-immediate-payments-data-sd.png" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"&gt;&lt;img border="0" data-original-height="900" data-original-width="1600" height="180" src="https://1.bp.blogspot.com/-qrlhtf3vujE/X2Ctmh0pIyI/AAAAAAAAxgo/jomUYBg9cak0uOrB7rxz1JjowWWVGwT4ACNcBGAsYHQ/s320/payments-immediate-payments-data-sd.png" width="320" /&gt;&lt;/a&gt;&lt;br /&gt;The second figure on the right here is labeled as an&amp;nbsp;&lt;i&gt;immediate payments data example &lt;/i&gt;and is meant to provide more insights into the movement of event streams and flow of data through the above described process of executing on a payments request. Exploring the details of this figure is left to the reader.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;Project examples&lt;/h3&gt;&lt;div&gt;Sharing the process results for our payments blueprint is what this series is about, but there are project artifacts and diagrams that can also be shared with you the reader. We've pulled together an&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/portfolio-architecture-examples" target="_blank"&gt;examples repository&lt;/a&gt;&amp;nbsp;for all our architecture blueprint diagrams.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;div style="text-align: right;"&gt;&lt;/div&gt;The&amp;nbsp;&lt;a href="https://gitlab.com/redhatdemocentral/portfolio-architecture-examples" target="_blank"&gt;Portfolio Architecture Examples&lt;/a&gt;&amp;nbsp;repository makes it possible to collect and share individual images from each diagram element as well as the entire project as a whole.&lt;/div&gt;&lt;div&gt;&lt;table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: right; margin-left: 1em; text-align: right;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-4t4sRfvBdlA/X2CrzVQ9sFI/AAAAAAAAxgY/vZ61Z75fKhk3GFBC3ZZlOyGpIJWtBgDngCNcBGAsYHQ/s1600/Screenshot%2B2020-09-15%2Bat%2B13.55.42.png" style="clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;"&gt;&lt;img alt="payments architecture" border="1" data-original-height="232" data-original-width="530" height="139" src="https://1.bp.blogspot.com/-4t4sRfvBdlA/X2CrzVQ9sFI/AAAAAAAAxgY/vZ61Z75fKhk3GFBC3ZZlOyGpIJWtBgDngCNcBGAsYHQ/s320/Screenshot%2B2020-09-15%2Bat%2B13.55.42.png" title="" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Figure 1 - Physical diagrams in example repository.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For example, if you scroll down to the file listings on the main page, you can locate all the example physical diagrams as shown in figure 1.&lt;br /&gt;&lt;div style="text-align: right;"&gt;&lt;/div&gt;&lt;br /&gt;This is the collection associated with payments:&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;in this case there are multiple images you can click to view&lt;/li&gt;&lt;li&gt;a project file you can download to your local machine using the&amp;nbsp;&lt;i&gt;Download Diagram&lt;/i&gt;&amp;nbsp;link&lt;/li&gt;&lt;li&gt;a&amp;nbsp;&lt;i&gt;Load Diagram&lt;/i&gt;&amp;nbsp;link that you can &lt;a href="https://redhatdemocentral.gitlab.io/portfolio-architecture-tooling/index.html?#/portfolio-architecture-examples/projects/schematic-diagrams-payments.drawio" target="_blank"&gt;click to automatically open the project diagrams&lt;/a&gt; in the diagram tooling used in this blueprint (use private or incognito browser mode to avoid caching issues and a smoother tooling experience)&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div&gt;Give it a try and feel free to explore the collection of logical, schematic, detailed, solution, and community diagrams. This should allow you to get started much quicker than from scratch if you can kick-start a project with existing diagrams.&lt;br /&gt;&lt;br /&gt;Should you desire to start designing your own diagrams, please contribute the project file (ending in .drawio) by raising an issue with the file attached. We'd love to continue collecting these projects for others to use.&lt;br /&gt;&lt;br /&gt;Finally, there is a free online&amp;nbsp;&lt;a href="https://redhatdemocentral.gitlab.io/portfolio-architecture-workshops" target="_blank"&gt;beginners guide workshop&lt;/a&gt;&amp;nbsp;available focused on using the diagram tooling, please explore to learn tips and tricks from the experts.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;What's next&lt;/h3&gt;&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;An overview of the series on the payments portfolio architecture blueprint can be found here:&lt;br /&gt;&lt;ol style="text-align: left;"&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/financial-payments-architecture-an-introduction.html" target="_blank"&gt;An introduction&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/payments-architecture-common-elements.html" target="_blank"&gt;Common architecture elements&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.schabell.org/2020/09/payments-architecture-immediate-payments-example.html" target="_blank"&gt;Immediate payments example&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Anti-money laundering example&lt;/li&gt;&lt;li&gt;Fraud detection example&lt;/li&gt;&lt;li&gt;Financial calculations example&lt;/li&gt;&lt;/ol&gt;&lt;ol style="text-align: left;"&gt;&lt;/ol&gt;Catch up on any articles you missed by following one of the links above.&lt;br /&gt;&lt;br /&gt;Next in this series, taking a look at the&amp;nbsp;generic&amp;nbsp;&lt;i&gt;anti-money laundering example&lt;/i&gt;&amp;nbsp;in a cloud-native architecture focused on payment processing.&lt;br /&gt;&lt;br /&gt;(Article co-authored by&amp;nbsp;&lt;a href="https://www.linkedin.com/in/ramonv/?originalSubdomain=uk" target="_blank"&gt;Ramon Villarreal&lt;/a&gt;)&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:63t7Ie-LG7Y"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=63t7Ie-LG7Y" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:qj6IDK7rITs"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?d=qj6IDK7rITs" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/schabell/jboss?a=7r9lufX4aUk:obELbUzacaU:gIN9vFwOqvQ"&gt;&lt;img src="http://feeds.feedburner.com/~ff/schabell/jboss?i=7r9lufX4aUk:obELbUzacaU:gIN9vFwOqvQ" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/schabell/jboss/~4/7r9lufX4aUk" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/IjfMUYbRB_w" height="1" width="1" alt=""/&gt;</content><summary>Part 3 - Immediate paymentsCloud technology is changing the way payment services are architectured. In this series we will be presenting insight from our customers on adopting open source and cloud technology to modernize their payment service. So far we've presented research-based architectural blueprints of omnichannel customer experience, integrating with SaaS applications, and cloud-native dev...</summary><dc:creator>Eric D. Schabell</dc:creator><dc:date>2020-09-24T05:00:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/7r9lufX4aUk/payments-architecture-immediate-payments-example.html</feedburner:origLink></entry><entry><title>Kubernetes: The evolution of distributed systems</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/u8IJBG1sQkc/" /><category term="DevNation" /><category term="Kubernetes" /><category term="Microservices" /><category term="Serverless" /><category term="application runtime" /><category term="distributed applications" /><category term="Kubernetes dapr" /><category term="mecha architecture" /><category term="service mesh" /><author><name>Bilgin Ibryam</name></author><id>https://developers.redhat.com/blog/?p=787807</id><updated>2020-09-23T07:00:38Z</updated><published>2020-09-23T07:00:38Z</published><content type="html">&lt;p&gt;DevNation Tech Talks are hosted by the Red Hat technologists who create our products. These sessions include real solutions plus code and sample projects to help you get started. In this talk, you’ll learn about Kubernetes and distributed systems from &lt;a href="https://developers.redhat.com/blog/author/bibryam/"&gt;Bilgin Ibryam&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/author/burrsutter/"&gt;Burr Sutter&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Cloud-native applications of the future will consist of hybrid workloads: stateful applications, batch jobs, stateless microservices, and functions (plus maybe something else) wrapped as Linux &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containers&lt;/a&gt; and deployed via &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; on any cloud. Functions and the so-called &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt; computing model are the latest evolution of what started as service-oriented architecture years ago. But is this the last step of the application architecture evolution and is it here to stay?&lt;/p&gt; &lt;p&gt;&lt;span id="more-787807"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;During this talk, we will take you on a journey exploring distributed application needs, and how they evolved with Kubernetes, &lt;a href="https://developers.redhat.com/topics/service-mesh/"&gt;Istio&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://knative.dev/"&gt;Knative&lt;/a&gt;, &lt;a target="_blank" rel="nofollow" href="https://dapr.io/"&gt;Dapr&lt;/a&gt;, and other projects. By the end of the session, you will know what is coming after &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Watch the entire talk:&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/CZPEIJFJV9k?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;Join us at an &lt;a href="https://developers.redhat.com/events/"&gt;upcoming developer event&lt;/a&gt;, and see our collection of &lt;a href="https://developers.redhat.com/devnation/?page=0"&gt;past DevNation Tech Talks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#38;linkname=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F09%2F23%2Fkubernetes-the-evolution-of-distributed-systems%2F&amp;#038;title=Kubernetes%3A%20The%20evolution%20of%20distributed%20systems" data-a2a-url="https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/" data-a2a-title="Kubernetes: The evolution of distributed systems"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/"&gt;Kubernetes: The evolution of distributed systems&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/u8IJBG1sQkc" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;DevNation Tech Talks are hosted by the Red Hat technologists who create our products. These sessions include real solutions plus code and sample projects to help you get started. In this talk, you’ll learn about Kubernetes and distributed systems from Bilgin Ibryam and Burr Sutter. Cloud-native applications of the future will consist of hybrid workloads: [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/"&gt;Kubernetes: The evolution of distributed systems&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">787807</post-id><dc:creator>Bilgin Ibryam</dc:creator><dc:date>2020-09-23T07:00:38Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/09/23/kubernetes-the-evolution-of-distributed-systems/</feedburner:origLink></entry></feed>
